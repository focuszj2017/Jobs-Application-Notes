# 面试查漏补缺

## 知识点

### Object类

包含的方法：

+ clone()
+ equals()
+ hashcode()
+ wait()
+ notify()
+ notifyAll()
+ toString()
+ finalize() 当GC确定不存在对该对象有更多引用时，由对象对垃圾回收器调用此方法
+ wait(long timeout)

### 关于volatie

volatie原理 ——来自脉脉

> 不知道为什么这么多人都说volatile是强行读主存。cpu提供了缓存一致性的保证(MESI)，一个cpu的缓存被修改，其他的cpu显然也可以感知。只是很多架构的cpu为了效率引入了invalid queue和发送队列，导致MESI协议只有在这两个队列被清空后才有效。所以volatile本质上是生成lock指令，这个指令把这些队列都清空了。不需要去主存读。

volatile在单例模式（双重检查）中的应用
1 volatile会在写操作前后插入内存屏障，防止指令重排序。

指令重排序：
JVM创建对象流程：1.给Singleton分配内存 2.调用构造函数，初始化零值 3.将Singleton指向分配的内存空间。

### 关于静态变量和静态方法

0. 对于静态变量、静态初始化块、变量、初始化块、构造器，它们的初始化顺序依次是（静态变量、静态初始化块）>（变量、初始化块）>构造器。

1. 加载的顺序：先父类的static成员变量 -> 子类的static成员变量 -> 父类的成员变量 -> 父类构造 -> 子类成员变量 -> 子类构造

2. static只会加载一次，所以通俗点讲第一次new的时候，所有的static都先会被全部载入(以后再有new都会忽略)，进行默认初始化。在从上往下进行显示初始化。这里静态代码块和静态成员变量没有先后之分，谁在上，谁就先初始化

3. 构造代码块是什么？把所有构造方法中相同的内容抽取出来，定义到构造代码块中，将来在调用构造方法的时候，会去自动调用构造代码块。构造代码快优先于构造方法。

kill

### IO多路复用

### 解决哈希冲突的三种方法

+ 拉链法

  HashMap，HashSet其实都是采用的拉链法来解决哈希冲突的，就是在每个位桶实现的时候，我们采用链表（jdk1.8之后采用链表+红黑树）的数据结构来去存取发生哈希冲突的输入域的关键字（也就是被哈希函数映射到同一个位桶上的关键字）。

+ 开放定址法

  开放地址法有个非常关键的特征，就是所有输入的元素全部存放在哈希表里，也就是说，位桶的实现是不需要任何的链表来实现的，换句话说，也就是这个哈希表的装载因子不会超过1。它的实现是在插入一个元素的时候，先通过哈希函数进行判断，若是发生哈希冲突，就以当前地址为基准，根据再寻址的方法（探查序列），去寻找下一个地址，若发生冲突再去寻找，直至找到一个为空的地址为止。所以这种方法又称为再散列法。

+ 再散列法

### 单例模式

+ 懒汉式

  ```java
  //在类加载时就完成了初始化，所以类加载较慢，但获取对象的速度快
  public class SingletonObject1 {
      // 利用静态变量来存储唯一实例
      private static final SingletonObject1 instance = new SingletonObject1();
  
      // 私有化构造函数
      private SingletonObject1(){
          // 里面可能有很多操作
      }
  
      // 提供公开获取实例接口
      public static SingletonObject1 getInstance(){
          return instance;
      }
  }
  
  ```

+ 饿汉式

  ```java
  //懒汉式（双重检查版本）
  public class Singleton {
      private volatile static Singleton uniqueInstance;
  
      //私有构造
      private Singleton() {
      }
  
      public static Singleton getInstance() {
          //为了提升效率，减少不必要锁竞争
          if (uniqueInstance == null) {
              synchronized (Singleton.class) {
                  //二次检验，避免创建多余实例
                  if ((uniqueInstance == null)) {
                      uniqueInstance = new Singleton();
                  }
              }
          }
          return uniqueInstance;
      }
  }
  ```

+ 枚举

  ```java
  /**
   * 枚举类实现单例模式
   */
  public class Singleton_Enum {
      private Singleton_Enum(){}
  
      private enum Singleton {
          INSTANCE;
  
          private final Singleton_Enum instance;
  
          private Singleton(){
              instance = new Singleton_Enum();
          }
  
          private Singleton_Enum getInstance() {
              return INSTANCE.instance;
          }
      }
  
      public static Singleton_Enum getInstance(){
          return Singleton.INSTANCE.getInstance();
      }
  }
  ```

+ 静态内部类

  ```java
  /**
   * 静态内部类实现单例模式
   */
  public class Singleton_StaticClass {
      private Singleton_StaticClass(){}
  
      private static class InstanceHolder{
          private static final Singleton_StaticClass instance = new Singleton_StaticClass();
      }
  
      public Singleton_StaticClass getInstance() {
          return InstanceHolder.instance;
      }
  
  }
  ```

### ArrayList

#### ArrayList扩容机制

+ add方法

  ```java
      /**
       * 将指定的元素追加到此列表的末尾。
       */
      public boolean add(E e) {
     //添加元素之前，先调用ensureCapacityInternal方法
          ensureCapacityInternal(size + 1);  // Increments modCount!!
          //这里看到ArrayList添加元素的实质就相当于为数组赋值
          elementData[size++] = e;
          return true;
      }
  ```

+ ensureCapacityInternal方法

  ```java
     //得到最小扩容量
      private void ensureCapacityInternal(int minCapacity) {
          if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {
                // 获取默认的容量和传入参数的较大值
              minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity);
          }
  
          ensureExplicitCapacity(minCapacity);
      }
  ```

  + 当我们要 add 进第 1 个元素到 ArrayList 时，elementData.length 为 0 （因为还是一个空的 list），因为执行了 `ensureCapacityInternal()` 方法 ，所以 minCapacity 此时为 **10**。此时，`minCapacity - elementData.length > 0`成立，所以会进入 `grow(minCapacity)` 方法。
  + 当 add 第 2 个元素时，minCapacity 为 2，此时 e lementData.length(容量)在添加第一个元素后扩容成 10 了。此时，`minCapacity - elementData.length > 0` 不成立，所以不会进入 （执行）`grow(minCapacity)` 方法。
  + 添加第 3、4···到第 10 个元素时，依然不会执行 grow 方法，数组容量都为 10。
  + 直到添加第 11 个元素，minCapacity(为 11)比 elementData.length（为 10）要大。进入 grow 方法进行扩容。

  使用grow方法，**int newCapacity = oldCapacity + (oldCapacity >> 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍左右（oldCapacity 为偶数就是 1.5 倍，否则是 1.5 倍左右）**

### HashMap

+ JDK 1.7之前采用数组+链表，JDK1.8之后采用数组+链表+红黑树
+ 默认初始化容量为16，之后每次扩容，容量变为原来的两倍
+ JDK1.8 以后的 HashMap 在解决哈希冲突时有了较⼤的变化，当链表⻓度 ⼤于阈值（默认为 8）（将链表转换成红⿊树前会判断，如果当前数组的⻓度⼩于 64，那么 会选择先进⾏数组扩容，⽽不是转换为红⿊树）时，将链表转化为红⿊树，以减少搜索时 间。Hashtable 没有这样的机制
+ **HashMap长度是2的幂次的原因**
  + “取余(%)操作中如果除数是2 的幂次则等价于与其除数减⼀的与(&)操作（也就是说 hash%length==hash&(length-1)的前提 是 length 是2的 n 次⽅；）。” 并且 采⽤⼆进制位操作 &，相对于%能够提⾼运算效率，这就解 释了 HashMap 的⻓度为什么是2的幂次⽅。

### ConcurrentHashMap

+ JDK1.7 之前采用分割分段（Segment）的概念；JDK 1.8后使用Node数组+链表+红黑树的结构，线程安全使用synchronized和CAS来保证

### 大数取余

+ n %= 1000000007(八个零)可以避免int32/int64越界

### PFLOPS

+ [FLOPS](https://baike.baidu.com/item/FLOPS/989494)是floating point operations per second每秒所执行的[浮点运算](https://baike.baidu.com/item/浮点运算)次数的英文缩写。
+ 10^15次浮点运算

### 秒杀系统中超卖问题

![preview](https://pic1.zhimg.com/v2-2da6e26351ec8edf41e3dd352ccacc1b_r.jpg?source=1940ef5c)

使用技术：URL动态化、Redis集群、主从同步、读写分离、Nginx、前端资源静态化、限流、库存预热、

限流&降级&熔断&隔离、削峰填谷（MQ）

### ARP协议

**ARP协议通过"一问一答"实现交互，但是"问"和"答"都有讲究，"问"是通过广播形式实现，"答"是通过单播形式。**

【ARP协议字段解读】

Hardware type ：硬件类型，标识链路层协议

Protocol type： 协议类型，标识网络层协议

Hardware size ：硬件地址大小，标识MAC地址长度，这里是6个字节（48bti）

Protocol size： 协议地址大小，标识IP地址长度，这里是4个字节（32bit）

Opcode： 操作代码，标识ARP数据包类型，1表示请求，2表示回应

Sender MAC address ：发送者MAC

Sender IP address ：发送者IP

Target MAC address ：目标MAC，此处全0表示在请求

Target IP address： 目标IP

### 三次握手四次挥手

**四次挥手释放连接时，等待2MSL的意义?**

两个理由：

+ 保证客户端发送的最后一个ACK报文段能够到达服务端。

这个ACK报文段有可能丢失，使得处于LAST-ACK状态的B收不到对已发送的FIN+ACK报文段的确认，服务端超时重传FIN+ACK报文段，而客户端能在2MSL时间内收到这个重传的FIN+ACK报文段，接着客户端重传一次确认，重新启动2MSL计时器，最后客户端和服务端都进入到CLOSED状态，若客户端在TIME-WAIT状态不等待一段时间，而是发送完ACK报文段后立即释放连接，则无法收到服务端重传的FIN+ACK报文段，所以不会再发送一次确认报文段，则服务端无法正常进入到CLOSED状态。

+ 防止“已失效的连接请求报文段”出现在本连接中。

客户端在发送完最后一个ACK报文段后，再经过2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，使下一个新的连接中不会出现这种旧的连接请求报文段。

### 数据库三大范式

#### 第一范式**

> **1NF的定义为：符合1NF的关系中的每个属性都不可再分**。

#### 第二范式

> 根据2NF的定义，判断的依据实际上就是看数据表中是否存在非主属性对于码的部分函数依赖。若存在，则数据表最高只符合1NF的要求，若不存在，则符合2NF的要求。

#### 第三范式

> 3NF在2NF的基础之上，消除了非主属性对于码的传递函数依赖。也就是说， 如果存在非主属性对于码的传递函数依赖，则不符合3NF的要求。

### 二维码扫码登录原理

#### **网页端+服务器端**

接下来就是对于这个服务的详细实现。首先，大概说一下原理：用户打开网站的登录页面的时候，向浏览器的服务器发送获取登录二维码的请求。

服务器收到请求后，随机生成一个uuid，将这个id作为key值存入redis服务器，同时设置一个过期时间，再过期后，用户登录二维码需要进行刷新重新获取。同时，将这个key值和本公司的验证字符串合在一起，通过二维码生成接口，生成一个二维码的图片（二维码生成，网上有很多现成的接口和源码，这里不再介绍。）然后，将二维码图片和uuid一起返回给用户浏览器。

浏览器拿到二维码和uuid后，会每隔一秒向浏览器发送一次，登录是否成功的请求。请求中携带有uuid作为当前页面的标识符。这里有的同学就会奇怪了，服务器只存了个uuid在redis中作为key值，怎么会有用户的id信息呢？

这里确实会有用户的id信息，这个id信息是由手机服务器存入redis中的。

#### **手机端+服务器**

话说，浏览器拿到二维码后，将二维码展示到网页上，并给用户一个提示：请掏出您的手机，打开扫一扫进行登录。用户拿出手机扫描二维码，就可以得到一个验证信息和一个uuid（扫描二维码获取字符串的功能在网上同样有很多demo，这里就不详细介绍了）。

由于手机端已经进行过了登录，在访问手机端的服务器的时候，参数中都回携带一个用户的token，手机端服务器可以从中解析到用户的userId（这里从token中取值而不是手机端直接传userid是为了安全，直接传userid可能会被截获和修改，token是加密的，被修改的风险会小很多）。

手机端将解析到的数据和用户token一起作为参数，向服务器发送验证登录请求（这里的服务器是手机服务器，手机端的服务器跟网页端服务器不是同一台服务器）。服务器收到请求后，首先对比参数中的验证信息，确定是否为用户登录请求接口。如果是，返回一个确认信息给手机端。

手机端收到返回后，将登录确认框显示给用户（防止用户误操作，同时使登录更加人性化）。用户确认是进行的登录操作后，手机再次发送请求。服务器拿到uuId和userId后，将用户的userid作为value值存入redis中以uuid作为key的键值对中。

#### **登录成功**

然后，浏览器再次发送请求的时候，浏览器端的服务器就可以得到一个用户Id，并调用登录的方法，声成一个浏览器端的token，再浏览器再次发送请求的时候，将用户信息返回给浏览器，登录成功。这里存储用户id而不是直接存储用户信息是因为，手机端的用户信息，不一定是和浏览器端的用户信息完全一致。

### http1.0与2.0

### 常用的几种线程池

**FixedThreadPool** :`FixedThreadPool` 被称为可重用固定线程数的线程池

**SingleThreadPool**:`SingleThreadExecutor` 是只有一个线程的线程池。

**CachedThreadPool**:`CachedThreadPool` 是一个会根据需要创建新线程的线程池。

## 并发

### 锁优化

这里的锁优化主要是指 JVM 对 synchronized 的优化

+ 自旋锁

+ 锁消除

+ 锁粗化

+ 轻量级锁

+ 偏向锁

  > 偏向锁的思想是偏向于让第一个获取锁对象的线程，这个线程在之后获取该锁就不再需要进行同步操作，甚至连 CAS 操作也不再需要。
  >
  > 当锁对象第一次被线程获得的时候，进入偏向状态，标记为 1 01。同时使用 CAS 操作将线程 ID 记录到 Mark Word 中，如果 CAS 操作成功，这个线程以后每次进入这个锁相关的同步块就不需要再进行任何同步操作。
  >
  > 当有另外一个线程去尝试获取这个锁对象时，偏向状态就宣告结束，此时撤销偏向（Revoke Bias）后恢复到未锁定状态或者轻量级锁状态。

## JVM

### Java内存区域

#### 运行时数据区域

1. 程序计数器
2. Java虚拟机栈
3. 本地方法栈
4. Java堆
5. 方法区
6. 运行时常量池
7. 直接内存

#### 关于方法区与运行时常量池

``` java
public class RuntimeConstantPoolOOM {
  public static void main(String[] args) {
      String str1 = new StringBuilder("计算机").append("软件").toString();
      System.out.println(str1.intern() == str1);
      String str2 = new StringBuilder("ja").append("va").toString();
      System.out.println(str2.intern() == str2); 
    }
}
```

> 这段代码在JDK 6中运行，会得到两个false，而在JDK 7中运行，会得到一个true和一个false。产 生差异的原因是，在JDK 6中，intern()方法会把首次遇到的字符串实例复制到永久代的字符串常量池 中存储，返回的也是永久代里面这个字符串实例的引用，而由St ringBuilder创建的字符串对象实例在 Java堆上，所以必然不可能是同一个引用，结果将返回false。
> 而JDK 7(以及部分其他虚拟机，例如JRockit)的intern()方法实现就不需要再拷贝字符串的实例 到永久代了，既然字符串常量池已经移到Java堆中，那只需要在常量池里记录一下首次出现的实例引用即可，因此intern()返回的引用和由StringBuilder创建的那个字符串实例就是同一个。而对str2比较返回false, 这是因为"java"这个字符串在执行StringBuilder.toString()之 前 就 已 经 出 现 过 了 ， 字 符 串 常 量 池中已经有它的引用，不符合intern()方法要求“首次遇到”的原则，“计算机软件”这个字符串则是首次 出现的，因此结果返回t rue。

**在大量使用反射，动态代理，CGLib等框架的场景，以及动态代理生成JSP和OSGI这类频繁自定义的场景都需要虚拟机具备类卸载的功能，**以保证永久代不会溢出。

#### **创建对象过程**

![Java创建对象的过程](https://snailclimb.gitee.io/javaguide/docs/java/jvm/pictures/java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F/Java%E5%88%9B%E5%BB%BA%E5%AF%B9%E8%B1%A1%E7%9A%84%E8%BF%87%E7%A8%8B.png)

#### **Java对象创建过程**

Step1：类加载检查

> 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。

Step2：分配内存

> 在**类加载检查**通过后，接下来虚拟机将为新生对象**分配内存**。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。**分配方式**有 **“指针碰撞”** 和 **“空闲列表”** 两种，**选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定**。
>
> **内存分配的两种方式：指针碰撞和空闲列表**
>
> **内存分配保证线程安全的两种方式：CAS+失败重试和TLAB**
> CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。
> TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配

![内存分配的两种方式](https://snailclimb.gitee.io/javaguide/docs/java/jvm/pictures/java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F.png)

Step3：初始化零值

> 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。

Step4：设置对象头

> 初始化零值完成之后，**虚拟机要对对象进行必要的设置**，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 **这些信息存放在对象头中。** 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。

对象头存储两方面数据：

+ 存储对象运行时的数据，包括哈希值，对象的GC分代年龄，锁状态等
+ 类型指针，即该对象是哪个类的实例。

Step5：

> 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，`<init>` 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 `<init>` 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。

### JVM底层对重写重载机制的实现原理



### JVM垃圾回收

#### 对象的内存布局

+ 对象头(Header)
  + MarkWord
    + 哈希码(HashCode)
    + GC分代年龄
    + 锁状态标志
    + 线程持有的锁
    + 偏向线程ID
    + 偏向时间戳
  + 类型指针
    + 句柄
    + 直接指针
+ 实例数据(Instance Data)
+ 对齐填充(Padding)

##### 如何判断一个类是无用的类

判断标准:
1.该类的所有实例均已回收，也就是Java堆中不存在该类的任何实例
2.加载该类的ClassLoader已经回收
3.该类对应的Class对象没有在任何地方被引用，无法在任何地方反射访问该类的方法

判断方法:
1.引用计数法（存在互相引用的问题）
2.可达性分析算法

**可成为GCroots的对象**
1.虚拟机栈栈帧中引用的对象
2.本地方法栈(JNI)中引用的对象
3.方法区类静态属性引用的对象，即Java的引用类型静态变量
4.方法去常量引用的对象，比如字符串常量池里的引用
5.所有被同步锁持有的对象

#### OOM排查思路

1 首先要确定是内存泄漏引起的还是内存溢出引起的。

  要解决这个内存区域的异常，常规的处理方法是首先通过内存映像分析工具(如Eclipse Memory Analyzer)对Dump出来的堆转储快照进行分析。第一步首先应确认内存中导致OOM的对象是否是必 要的，也就是要先分清楚到底是出现了内存泄漏(Memory Leak)还是内存溢出(Memory Overflow)。

2 如果是内存泄漏

  可进一步通过工具查看泄漏对象到GC Roots的引用链，找到泄漏对象是通过怎 样的引用路径、与哪些GC Roots相关联，才导致垃圾收集器无法回收它们，根据泄漏对象的类型信息 以及它到GC Roots引用链的信息，一般可以比较准确地定位到这些对象创建的位置，进而找出产生内 存泄漏的代码的具体位置。

3 如果不是内存泄漏

  如果不是内存泄漏，换句话说就是内存中的对象确实都是必须存活的，那就应当检查Java虚拟机 的堆参数(-Xmx与-Xms)设置，与机器的内存对比，看看是否还有向上调整的空间。再从代码上检查 是否存在某些对象生命周期过长、持有状态时间过长、存储结构设计不合理等情况，尽量减少程序运 行期的内存消耗。

#### **判断一个对象已经无效的两种算法**

+ 引用计数法
+ 可达性分析算法

#### 可以作为GCRoots的对象

+ 虚拟机栈（栈帧中的本地变量表）中引用的对象
+ 本地方法栈（Native方法）中引用的对象
+ 方法区中类静态属性引用的对象
+ 方法区中常类引用的对象
+ 所有被同步锁持有的对象

#### **四种引用：**

 1．强引用（StrongReference）

以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于**必不可少的生活用品**，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。

2．软引用（SoftReference）

如果一个对象只具有软引用，那就类似于**可有可无的生活用品**。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。

软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。

3．弱引用（WeakReference）

如果一个对象只具有弱引用，那就类似于**可有可无的生活用品**。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。

弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。

4．虚引用（PhantomReference）

"虚引用"顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。

**虚引用主要用来跟踪对象被垃圾回收的活动**。

**虚引用与软引用和弱引用的一个区别在于：** 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。

特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为**软引用可以加速 JVM 对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生**。

> 面试问题：HotSpot 为什么要分为新生代和老年代？
>
> 答：将 Java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。比如在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。

#### 频繁FullGC

**频繁Full-GC如何排查？**

> 总结：
>
> 默认情况下，System.gc()会显式直接触发Full GC，同时对老年代和新生代进行回收。而一般情况下我们认为，垃圾回收应该是自动进行的，无需手工触发。如果过于频繁地触发垃圾回收对系统性能是没有好处的。因此虚拟机提供了一个参数DisableExplicitGC来控制是否手工触发GC。应该从根源解决对象生成的问题，而不是要生成之后再去回收，这样很耗CPU。

#### Full GC 的触发条件

> **触发MinorGC（Young GC）**
>
> 虚拟机在进行minorGC之前会判断老年代最大的可用连续空间是否大于新生代的所有对象总空间
>
> + 如果大于的话，直接执行minorGC
> + 如果小于的话，判断是否开启HandlerPromotionFailure，没有开直接FullGC
> + 如果开了HandlerPromotionFailure,JVM会判断老年代的最大连续空间是否大于历次晋升的大小，如果小于直接执行FullGC
> + 如果大于的话，执行minorGC
>
> **触发FullGC**
>
> **1. 显示调用 System.gc()**
>
> 只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。
>
> **2. 老年代空间不足**
>
> 老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。
>
> 为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对象进入老年代的年龄，让对象在新生代多存活一段时间。
>
> **3. 持久代空间不足**
>
> 如果有持久代的话（永久代），系统当中需要加载的类，调用的方法很多，同时持久代当中没有足够的空间，就会触发一次FullGC
>
> **4. YGC出现promotion failure**
>
> promotion failure发生在Young GC，如果Survivor区当中存活对象的年龄达到了设定值，就会将Survivor区当中的对象拷贝到老年代，如果老年代的空间不足，就会发生promotion failure，接下来就会发生Full GC
>
> **5. 统计YGC发生时候晋升到老年代的平均总大小小于老年代的空闲空间**
>
> 在发生YGC时会判断，是否安全。这里安全指的是，当前老年代空间可以容纳YGC晋升对象的平均大小，如果不安全，就不会执行YGC，转而执行FullGC

#### 垃圾收集与回收

Stop the world

  Stop the world发生在移动存活对象的场景下。例如老年代每次垃圾回收后都有大量的存活对象区域，移动存活对象并更新所有引用这些对象的地方是一种极为负重的操作，而且这种对象移动操作必须全程暂停用户进程应用程序才能进行。因此被称为Stop the world.

  现有的垃圾收集器在第一步可达性分析算法，根节点枚举时都会用到Stop the world

  采用Stop the world的收集器一般基于标记-整理或标记-复制算法，优点在于吞吐量高，但用户体验较差。

  而基于标记清除算法但收集器如CMS，G1，ZGC使用stop the world就较少，优点是重视用户体验，停顿时间较短，但与此同时吞吐量也相对较差。

##### 垃圾收集器

1 Serial收集器

  Serial收集是个“单线程工作的收集器”，但它的“单线程”的意义并不仅仅是说明它只会使用一个处理器或一条收集线程去完成垃圾收集工作，更重要的是强 调在它进行垃圾收集时，必须暂停其他所有工作线程，直到它收集结束。

2 ParNew收集器

  ParNew收集器实质上是Serial收集器的多线程并行版本，除了同时使用多条线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数

  生产环境中多使用ParNew收集器和CMS收集搭配，因为CMS是面向老年代的垃圾收集器，ParNew面向新生代。

3 Parallel Scavenge收集器

  Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CM S等收集器的关注点是尽可能 地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐 量(Throughput)。所谓吞吐量就是处理器用于运行用户代码的时间与处理器总消耗时间的比值。

4 CMS收集器

  CMS收集器基于标记清除算法。运作流程为：

    1. 初始标记
    2. 并发标记
    3. 重新标记
    4. 并发清除

  其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快;并发标记阶段就是从GC Roots的直接关联对象开始遍历整个对 象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行;而重 新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的 标记记录(详见3.4.6节中关于增量更新的讲解)，这个阶段的停顿时间通常会比初始标记阶段稍长一 些，但也远比并发标记阶段的时间短;最后是并发清除阶段，清理删除掉标记阶段判断的已经死亡的 对象，由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的。

  由于在整个过程中耗时最长的并发标记和并发清除阶段中，垃圾收集器线程都可以与用户线程一 起工作，所以从总体上来说，CM S收集器的内存回收过程是与用户线程一起并发执行的。通过图3-11 可以比较清楚地看到CMS收集器的运作步骤中并发和需要停顿的阶段。

  CMS收集器也存在着一些的缺点；

    1. **CMS对于处理器资源很敏感**，在小于四核处理器的情况，并发清除占用的线程会很高，就可能导致用户程序执行速度忽然大幅降低。
    2. **CMS无法处理浮动垃圾**，可能能出现Con-current Mode Failure(并发失败)，进而进行一次Stop the world的Full GC。注：浮动垃圾是指并发清理时，用户线程仍在进行产生的垃圾对象，只能留在下一次垃圾收集中处理

5 G1收集器

  虽然G1仍然保留新生代和老年代的概念，但新生代和老年代不再是固定的了，它们都是一系列区 域(不需要连续)的动态集合。G1收集器之所以能建立可预测的停顿时间模型，是因为它将Region作 为单次回收的最小单元，即每次收集到的内存空间都是Region大小的整数倍，这样可以有计划地避免 在整个Java堆中进行全区域的垃圾收集。更具体的处理思路是让G1收集器去跟踪各个Region里面的垃 圾堆积的“价值”大小，价值即回收所获得的空间大小以及回收所需时间的经验值，然后在后台维护一 个优先级列表，每次根据用户设定允许的收集停顿时间(使用参数-XX:M axGCPauseM illis指定，默 认值是200毫秒)，优先处理回收价值收益最大的那些Region，这也就是“Garbage First”名字的由来。 这种使用Region划分内存空间，以及具有优先级的区域回收方式，保证了G1收集器在有限的时间内获 取尽可能高的收集效率。

  G1收集器垃圾回收流程

    1. 初始标记
       1. 标记一下GCRoots直接关联到的对象，并且修改TAMS指针的值，该阶段需要停顿线程，但耗时很短。
    2. 并发标记
       1. 从GC Root开始对堆中对象进行可达性分析，递归扫描整个堆 里的对象图，找出要回收的对象，这阶段耗时较长，但可与用户程序并发执行。当对象图扫描完成以 后，还要重新处理SAT B记录下的在并发时有引用变动的对象。
    3. 最终标记
       1. 对用户线程做另一个短暂的暂停，用于处理并发阶段结束后仍遗留 下来的最后那少量的SATB记录。
    4. 筛选回收
       1. 负责更新Region的统计数据，对多个Region的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个Region构成回收集，然后把决定回收的那个Region中的存活对象移动到空的Region中，再清理掉旧的Region的全部空间 。这里的操作涉及存活对象的移动，是必须暂停用户线程，由多条收集器线程并行完成的。

  因此，**与CMS收集器的回收流程**相比，CMS只有初始标记、重新标记阶段需要Stop the world，而G1在初始标记、最终标记、筛选回收三个阶段都需要Stop the world。这也可以体现出G1并不是完全为了用户设计，G1主要面向的还是低延迟，保证吞吐量的方案。

#### G1（Garbage First）垃圾回收流程

> G1 GC的垃圾回收过程主要包括如下三个环节：
>
> + 年轻代GC（Young GC）
> + 老年代并发标记过程（Concurrent Marking）
> + 混合回收（Mixed GC）
> + （如果需要，单线程、独占式、高强度的Full GC还是继续存在的。它针对GC的评估失败提供了一种失败保护机制，即强力回收。）
>
> 顺时针，Young GC --> Young GC+Concurrent Marking --> Mixed GC顺序，进行垃圾回收
>
> **回收流程**
>
> 1. 应用程序分配内存，当年轻代的Eden区用尽时开始年轻代回收过程；G1的年轻代收集阶段是一个并行的独占式收集器。在年轻代回收期，G1 GC暂停所有应用程序线程，启动多线程执行年轻代回收。然后从年轻代区间移动存活对象到Survivor区间或者老年区间，也有可能是两个区间都会涉及。
> 2. 当堆内存使用达到一定值（默认45%）时，开始老年代并发标记过程。
> 3. 标记完成马上开始混合回收过程。对于一个混合回收期，G1 GC从老年区间移动存活对象到空闲区间，这些空闲区间也就成为了老年代的一部分。和年轻代不同，老年代的G1回收器和其他GC不同，**G1的老年代回收器不需要整个老年代被回收，一次只需要扫描/回收一小部分老年代的Region就可以了**。同时，这个老年代Region是和年轻代一起被回收的。
> 4. 举个例子：一个Web服务器，Java进程最大堆内存为4G，每分钟响应1500个请求，每45秒钟会新分配大约2G的内存。G1会每45秒钟进行一次年轻代回收，每31个小时整个堆的使用率会达到45%，会开始老年代并发标记过程，标记完成后开始四到五次的混合回收。

#### 内存分配与回收策略分配

+ 对象优先在Eden区分配
+ 大对象直接进入老年代
+ 长期存活的对象将进入老年代
+ 动态对象年龄判定
  + 为了能更好地适应不同程序的内存状况，HotSpot虚拟机并不是永远要求对象的年龄必须达到- XX:M axTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于 Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到-XX:
    MaxTenuringThreshold中要求的年龄。
+ 空间分配担保
  + 在发生Minor GC之前，虚拟机必须先检查老年代最大可用的连续空间是否大于新生代所有对象总 空间，如果这个条件成立，那这一次Minor GC可以确保是安全的。如果不成立，则虚拟机会先查看- XX:HandlePromotionFailure参数的设置值是否允许担保失败(Handle Promotion Failure);如果允 许，那会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大 于，将尝试进行一次Minor GC，尽管这次Minor GC是有风险的;如果小于，或者-XX: HandlePromotionFailure设置不允许冒险，那这时就要改为进行一次Full GC。

#### 频繁FullGC但老年代未下降

> 关于什么情况下可能会引起 Full GC 频繁发生，老年代使用比例持续居高不下，原因如下
>
> 1、 大对象直接进入老年代因此频繁 Full GC；
> 2、 静态变量过多，变量生命周期过长，存活率过高，造成老年代过大；
> 3、 内存泄露导致无用对象晋升老年代并无法被 GC 回收。
>
> 第二点和第三点的可能性比较大。

### JDK命令行工具

jps：查看所有的Java进程

jstat：监视虚拟机各种运行状态信息

jinfo：实时地查看和调整虚拟机各项参数

jmap：生成堆转储快照

jhat：分析heapdump文件

jstack：生成虚拟机当前时刻的线程快照

### 类文件结构

Class文件结构：魔数、Class文件版本、常量池、访问标志、当前类索引父索引与接口索引、字段表集合、方法表集合、属性表集合

**常量池：**常量池中主要存放字面量和符号引用，字面量比较接近于 Java 语言层面的的常量概念，如文本字符串、声明为 final 的常量值等。而符号引用则属于编译原理方面的概念。包括下面三类常量：

+ 类和接口的全限定名
+ 字段的名称和描述符
+ 方法的名称和描述符

### 类加载过程

#### **类的生命周期**

![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B-%E5%AE%8C%E5%96%84.png)

#### **类加载过程**

![img](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B.png)

### 双亲委派模型

#### **定义**

每一个类都有一个对应它的类加载器。系统中的 ClassLoder 在协同工作的时候会默认使用 **双亲委派模型** 。即在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，首先会把该请求委派该父类加载器的 `loadClass()` 处理，因此所有的请求最终都应该传送到顶层的启动类加载器 `BootstrapClassLoader` 中。当父类加载器无法处理时，才由自己来处理。当父类加载器为null时，会使用启动类加载器 `BootstrapClassLoader` 作为父类加载器。

**[双亲委派模型的好处](https://snailclimb.gitee.io/javaguide/#/docs/java/jvm/类加载器?id=双亲委派模型的好处)**

双亲委派模型保证了Java程序的稳定运行，可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。如果没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 `java.lang.Object` 类的话，那么程序运行的时候，系统就会出现多个不同的 `Object` 类。

#### **用户自定义类加载器**

**[什么时候需要自定义类加载器？](https://youthlql.gitee.io/javayouth/#/docs/Java/JVM/JVM系列-第2章-类加载子系统?id=什么时候需要自定义类加载器？)**

在Java的日常应用程序开发中，类的加载几乎是由上述3种类加载器相互配合执行的，在必要时，我们还可以自定义类加载器，来定制类的加载方式。那为什么还需要自定义类加载器？

1. 隔离加载类（比如说我假设现在Spring框架，和RocketMQ有包名路径完全一样的类，类名也一样，这个时候类就冲突了。不过一般的主流框架和中间件都会自定义类加载器，实现不同的框架，中间价之间是隔离的）
2. 修改类加载的方式
3. 扩展加载源（还可以考虑从数据库中加载类，路由器等等不同的地方）
4. 防止源码泄漏（对字节码文件进行解密，自己用的时候通过自定义类加载器来对其进行解密

#### **static变量和static代码块**

1. static变量会在类加载过程——链接阶段——prepare阶段中分配内存并初始化零值。（被final修饰的static变量会在编译期就分配好初始值）

2. static变量被正式赋值是在类加载过程——初始化阶段

3. static代码块执行也是在初始化阶段

4. > <clinit>() 是由编译器自动收集类中所有类变量的赋值动作和静态语句块中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。例如以下代码：
   >
   > ```java
   > public class Test {
   >  static {
   >      i = 0;                // 给变量赋值可以正常编译通过
   >      System.out.print(i);  // 这句编译器会提示“非法向前引用”
   >  }
   >  static int i = 1;
   > }
   > ```

#### ClassLoader与JDBC

##### **需要破坏双亲委派模型**

当我们加载jdbc.jar 用于实现数据库连接的时候

1. 我们现在程序中需要用到SPI接口，而SPI接口属于rt.jar包中Java核心api
2. 然后使用双亲委派机制，引导类加载器把rt.jar包加载进来，而rt.jar包中的SPI存在一些接口，接口我们就需要具体的实现类了
3. 具体的实现类就涉及到了某些第三方的jar包了，比如我们加载SPI的实现类jdbc.jar包【首先我们需要知道的是 jdbc.jar是基于SPI接口进行实现的】
4. 第三方的jar包中的类属于系统类加载器来加载
5. 从这里面就可以看到SPI核心接口由引导类加载器来加载，SPI具体实现类由系统类加载器来加载

![img](https://cdn.jsdelivr.net/gh/youthlql/lqlp@v1.0.0/JVM/chapter_002/0022.png)

### 类初始化时机

#### 1.主动引用

虚拟机规范中并没有强制约束何时进行加载，但是规范严格规定了有且只有下列五种情况必须对类进行初始化（加载、验证、准备都会随之发生）：

+ 遇到 new、getstatic、putstatic、invokestatic 这四条字节码指令时，如果类没有进行过初始化，则必须先触发其初始化。最常见的生成这 4 条指令的场景是：使用 new 关键字实例化对象的时候；读取或设置一个类的静态字段（被 final 修饰、已在编译期把结果放入常量池的静态字段除外）的时候；以及调用一个类的静态方法的时候。
+ 使用 java.lang.reflect 包的方法对类进行反射调用的时候，如果类没有进行初始化，则需要先触发其初始化。
+ 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。
+ 当虚拟机启动时，用户需要指定一个要执行的主类（包含 main() 方法的那个类），虚拟机会先初始化这个主类；
+ 当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic, REF_putStatic, REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化；

#### 2.被动引用

以上 5 种场景中的行为称为对一个类进行主动引用。除此之外，所有引用类的方式都不会触发初始化，称为被动引用。被动引用的常见例子包括：

- 通过子类引用父类的静态字段，不会导致子类初始化。

```java
System.out.println(SubClass.value);  // value 字段在 SuperClass 中定义
```

- 通过数组定义来引用类，不会触发此类的初始化。该过程会对数组类进行初始化，数组类是一个由虚拟机自动生成的、直接继承自 Object 的子类，其中包含了数组的属性和方法。

```java
SuperClass[] sca = new SuperClass[10];
```

- 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。

```java
System.out.println(ConstClass.HELLOWORLD);
```

### SPI机制

![img](https://pic1.zhimg.com/80/v2-5210ed7234f7046273104bed9bfd7764_720w.png)

核心思路是底层使用ServiceLoad.load(xx.class)；方法，准备好实现的实现类全类名放在配置文件中，从而实现可插拔的spi机制。

![img](https://pic3.zhimg.com/80/v2-9db1c292ae743dad89615c2a41d967b6_720w.png)

## 操作系统

### **虚拟内存**

**定义：**虚拟内存**是计算机系统内存管理的一种技术，我们可以手动设置自己电脑的虚拟内存。不要单纯认为虚拟内存只是“使用硬盘空间来扩展内存“的技术。**虚拟内存的重要意义是它定义了一个连续的虚拟地址空间**，并且 **把内存扩展到硬盘空间**。

**虚拟内存的技术实现：**请求分页存储管理、请求分段存储管理、请求端页式存储管理

**请求分页与分页存储管理的区别：**它们之间的根本区别在于是否将一作业的全部地址空间同时装入主存。请求分页存储管理不要求将作业全部地址空间同时装入主存。基于这一点，请求分页存储管理可以提供虚存，而分页存储管理却不能提供虚存。

### 进程管理

#### 进程和线程

进程和线程的区别是什么？

1 拥有资源
  进程是资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源。
  资源包括**内存空间**，**I/O设备**
2 调度
  线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。
3 系统开销
  进程切换涉及到当前进程CPU环境的保存以及新的CPU环境的设置，系统开销大；而线程切换只需要保存和设置少量寄存器，开销很小。
4 通信方面
  线程间可以通过读写统一进程中的数据进行通信，而进程间通信要借助IPC（进程间通信）

#### 进程同步

1. 临界区

> 对临界资源进行访问的那段代码称为临界区。
>
> 为了互斥访问临界资源，每个进程在进入临界区之前，需要先进行检查。

1. 同步与互斥

> - 同步：多个进程因为合作产生的直接制约关系，使得进程有一定的先后执行关系。
> - 互斥：多个进程在同一时刻只有一个进程能进入临界区。

1. 信号量（Semaphore）
2. 管程：使用条件变量的wait/notify方法

#### 线程同步方法

> 1. 互斥量（Mutex）如：synchronized，lock等机制
> 2. 信号量（Semaphares） 它允许同⼀时刻多个线程访问同⼀资源，但是需要控制同⼀时刻访 问此资源的最⼤线程数量
> 3. 事件（event）Wait/Notify：通过通知操作的⽅式来保持多线程同步，还可以⽅便的实现多线 程优先级的⽐较操

#### 进程调度算法

1. 批处理系统
   1. 先来先服务（FCFS）非抢占式
   2. 短作业优先（SJF）非抢占式
   3. 最短剩余时间优先（SRTN）抢占式
2. 交互式系统
   1. 时间片轮转
   2. 优先级调度
   3. 多级反馈队列
3. 实时系统

#### 进程间的通信方式

1. 管道/匿名管道（Pipes）

   > - **管道的实质：**
   >   管道的实质是一个内核缓冲区，进程以先进先出的方式从缓冲区存取数据，管道一端的进程顺序的将数据写入缓冲区，另一端的进程则顺序的读出数据。
   >   该缓冲区可以看做是一个循环队列，读和写的位置都是自动增长的，不能随意改变，一个数据只能被读一次，读出来以后在缓冲区就不复存在了。
   >   当缓冲区读空或者写满时，有一定的规则控制相应的读进程或者写进程进入等待队列，当空的缓冲区有新数据写入或者满的缓冲区有数据读出来时，就唤醒等待队列中的进程继续读写。
   >
   >   **管道的局限：**
   >    管道的主要局限性正体现在它的特点上：
   >
   >   - 只支持单向数据流；
   >   - 只能用于具有亲缘关系的进程之间；
   >   - 没有名字；
   >   - 管道的缓冲区是有限的（管道制存在于内存中，在管道创建时，为缓冲区分配一个页面大小）；
   >   - 管道所传送的是无格式字节流，这就要求管道的读出方和写入方必须事先约定好数据的格式，比如多少字节算作一个消息（或命令、或记录）等等；

2. 有名管道（Names Pipes）

   > 有名管道不同于匿名管道之处在于它提供了一个路径名与之关联，**以有名管道的文件形式存在于文件系统中**，这样，**即使与有名管道的创建进程不存在亲缘关系的进程，只要可以访问该路径，就能够彼此通过有名管道相互通信**，因此，通过有名管道不相关的进程也能交换数据。值的注意的是，有名管道严格遵循**先进先出(first in first out)**,对匿名管道及有名管道的读总是从开始处返回数据，对它们的写则把数据添加到末尾。它们不支持诸如lseek()等文件定位操作。**有名管道的名字存在于文件系统中，内容存放在内存中。**

3. 信号（Signal）

   > **信号来源**
   > 信号是软件层次上对中断机制的一种模拟，是一种异步通信方式，，信号可以在用户空间进程和内核之间直接交互，内核可以利用信号来通知用户空间的进程发生了哪些系统事件，信号事件主要有两个来源：
   >
   > - 硬件来源：用户按键输入`Ctrl+C`退出、硬件异常如无效的存储访问等。
   > - 软件终止：终止进程信号、其他进程调用kill函数、软件异常产生信号。

4. 消息队列（Message Queueing）

   > **消息队列与管道、有名管道的区别**
   >
   > 与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。
   >
   > 另外与管道不同的是，消息队列在某个进程往一个队列写入消息之前，并不需要另外某个进程在该队列上等待消息的到达
   >
   > **消息队列特点总结：**
   > （1）消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识.
   > （2）消息队列允许一个或多个进程向它写入与读取消息.
   > （3）管道和消息队列的通信数据都是先进先出的原则。
   > （4）消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比FIFO更有优势。
   > （5）消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺。
   > （6）目前主要有两种类型的消息队列：POSIX消息队列以及System V消息队列，系统V消息队列目前被大量使用。系统V消息队列是随内核持续的，只有在内核重起或者人工删除时，该消息队列才会被删除。

5. 信号量（Semaphore）

   > **`Semaphore`(信号量)可以指定多个线程同时访问某个资源。（Java中实现）**
   >
   > 信号量（Semaphore）是一个整型变量，可以对其执行 down 和 up 操作，也就是常见的 P 和 V 操作。
   >
   > - **down** : 如果信号量大于 0 ，执行 -1 操作；如果信号量等于 0，进程睡眠，等待信号量大于 0；
   > - **up** ：对信号量执行 +1 操作，唤醒睡眠的进程让其完成 down 操作。
   >
   > down 和 up 操作需要被设计成原语，不可分割，通常的做法是在执行这些操作的时候屏蔽中断。
   >
   > 如果信号量的取值只能为 0 或者 1，那么就成为了 **互斥量（Mutex）** ，0 表示临界区已经加锁，1 表示临界区解锁

6. 共享内存（Shared memory）

7. 套接字（Socket）

**使用信号量实现生产者-消费者问题**

> 问题描述：使用一个缓冲区来保存物品，只有缓冲区没有满，生产者才可以放入物品；只有缓冲区不为空，消费者才可以拿走物品。
>
> 因为缓冲区属于临界资源，因此需要使用一个互斥量 mutex 来控制对缓冲区的互斥访问。
>
> 为了同步生产者和消费者的行为，需要记录缓冲区中物品的数量。数量可以使用信号量来进行统计，这里需要使用两个信号量：empty 记录空缓冲区的数量，full 记录满缓冲区的数量。其中，empty 信号量是在生产者进程中使用，当 empty 不为 0 时，生产者才可以放入物品；full 信号量是在消费者进程中使用，当 full 信号量不为 0 时，消费者才可以取走物品。
>
> 注意，不能先对缓冲区进行加锁，再测试信号量。也就是说，不能先执行 down(mutex) 再执行 down(empty)。如果这么做了，那么可能会出现这种情况：生产者对缓冲区加锁后，执行 down(empty) 操作，发现 empty = 0，此时生产者睡眠。消费者不能进入临界区，因为生产者对缓冲区加锁了，消费者就无法执行 up(empty) 操作，empty 永远都为 0，导致生产者永远等待下，不会释放锁，消费者因此也会永远等待下去。
>
> ```C++
> #define N 100typedef int semaphore;semaphore mutex = 1;semaphore empty = N;semaphore full = 0;void producer() { while(TRUE) {     int item = produce_item();     down(&empty);     down(&mutex);     insert_item(item);     up(&mutex);     up(&full); }}void consumer() { while(TRUE) {     down(&full);     down(&mutex);     int item = remove_item();     consume_item(item);     up(&mutex);     up(&empty); }}
> ```

#### 协程

> 代码中创建了一个叫做consumer的协程，并且在主线程中生产数据，协程中消费数据。
>
> 其中 **yield** 是python当中的语法。当协程执行到yield关键字时，会暂停在那一行，等到主线程调用send方法发送了数据，协程才会接到数据继续执行。
>
> 但是，yield让协程暂停，和线程的阻塞是有本质区别的。协程的暂停完全由程序控制，线程的阻塞状态是由操作系统内核来进行切换。
>
> 因此，**协程的开销远远小于线程的开销。**

### 死锁

#### 必要条件

1. 互斥：每个资源要么就是已经分配给了一个进程，要么就是可用的
2. 占有和等待：已经得到某个资源的进程可以再请求新的资源
3. 不可抢占：已经分配给了一个进程的资源不能强制性的抢占，只能被占有它的线程显示释放
4. 环路等待：有两个或者两个以上的进程组成一条环路，该环路中的每个进程都在等待下一个进程所占有的资源。

**1. 每种类型一个资源的死锁检测**

![image-20210504220832540](C:\Users\95845\AppData\Roaming\Typora\typora-user-images\image-20210504220832540.png)

每种类型一个资源的死锁检测算法是通过检测有向图是否存在环来实现，从一个节点出发进行深度优先搜索，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁的发生。

**2.每种类型多个资源的死锁检测**

每个进程最开始时都不被标记，执行过程有可能被标记。当算法结束时，任何没有被标记的进程都是死锁进程。

1. 寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。
2. 如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。
3. 如果没有这样一个进程，算法终止

**3.死锁恢复**

- 利用抢占恢复
- 利用回滚恢复
- 通过杀死进程恢复

### 内存管理

#### 虚拟内存

> 虚拟内存的目的是为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存。
>
> 为了更好的管理内存，操作系统将内存抽象成地址空间。每个程序拥有自己的地址空间，这个地址空间被分割成多个块，每一块称为一页。这些页被映射到物理内存，但不需要映射到连续的物理内存，也不需要所有页都必须在物理内存中。当程序引用到不在物理内存中的页时，由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的指令。
>
> 从上面的描述中可以看出，虚拟内存允许程序不用将地址空间中的每一页都映射到物理内存，也就是说一个程序不需要全部调入内存就可以运行，这使得有限的内存运行大程序成为可能。例如有一台计算机可以产生 16 位地址，那么一个程序的地址空间范围是 0~64K。该计算机只有 32KB 的物理内存，虚拟内存技术允许该计算机运行一个 64K 大小的程序。
>
> ![img](D:\Typora\68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f37623238316231652d303539352d343032622d616533352d3863393130383463333363312e706e67)

#### 分页系统地址映射

> 内存管理单元（MMU）管理着地址空间和物理内存的转换，其中的页表（Page table）存储着页（程序地址空间）和页框（物理内存空间）的映射表。
>
> 一个虚拟地址分成两个部分，一部分存储页面号，一部分存储偏移量。
>
> 下图的页表存放着 16 个页，这 16 个页需要用 4 个比特位来进行索引定位。例如对于虚拟地址（0010 000000000100），前 4 位是存储页面号 2，读取表项内容为（110 1），页表项最后一位表示是否存在于内存中，1 表示存在。后 12 位存储偏移量。这个页对应的页框的地址为 （110 000000000100）。
>
> ![img](D:\Typora\68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f63663433383661312d353863392d346563612d613137662d6531326231653937373065622e706e67)

#### 页面置换算法

**1.最佳**

> OPT, Optimal replacement algorithm

所选择的被换出的页面将是最长时间内不再被访问，通常可以保证获得最低的缺页率。

是一种理论上的算法，因为无法知道一个页面多长时间不再被访问。

**2.最近最久未使用（LRU）**

> LRU, Least Recently Used

虽然无法知道将来要使用的页面情况，但是可以知道过去使用页面的情况。LRU 将最近最久未使用的页面换出。

为了实现 LRU，需要在内存中维护一个所有页面的链表。当一个页面被访问时，将这个页面移到链表表头。这样就能保证链表表尾的页面是最近最久未访问的。

因为每次访问都需要更新链表，因此这种方式实现的 LRU 代价很高。

![img](D:\Typora\68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f65623835393232382d633066322d346263652d393130642d6439663736393239333532622e706e67)

**3.最近未使用**

> NRU, Not Recently Used

每个页面都有两个状态位：R 与 M，当页面被访问时设置页面的 R=1，当页面被修改时设置 M=1。其中 R 位会定时被清零。可以将页面分成以下四类：

- R=0，M=0
- R=0，M=1
- R=1，M=0
- R=1，M=1

当发生缺页中断时，NRU 算法随机地从类编号最小的非空类中挑选一个页面将它换出。

NRU 优先换出已经被修改的脏页面（R=0，M=1），而不是被频繁使用的干净页面（R=1，M=0）。

**4. 先进先出**

> FIFO, First In First Out

选择换出的页面是最先进入的页面。

该算法会将那些经常被访问的页面换出，导致缺页率升高。

**5. 第二次机会算法**

FIFO 算法可能会把经常使用的页面置换出去，为了避免这一问题，对该算法做一个简单的修改：

当页面被访问 (读或写) 时设置该页面的 R 位为 1。需要替换的时候，检查最老页面的 R 位。如果 R 位是 0，那么这个页面既老又没有被使用，可以立刻置换掉；如果是 1，就将 R 位清 0，并把该页面放到链表的尾端，修改它的装入时间使它就像刚装入的一样，然后继续从链表的头部开始搜索。

#### 常见的内存管理机制

1. 块式管理
2. 页式管理
3. 段式管理
4. 段页式管理机制

#### 快表和多级页表

在分⻚内存管理中，很重要的两点是： 1. 虚拟地址到物理地址的转换要快。 2. 解决虚拟地址空间⼤，⻚表也会很⼤的问题。

**快表**
为了解决虚拟地址到物理地址的转换速度，操作系统在 ⻚表⽅案 基础之上引⼊了 快表 来加速虚 拟地址到物理地址的转换。我们可以把快表理解为⼀种特殊的⾼速缓冲存储器（Cache），其中 的内容是⻚表的⼀部分或者全部内容。作为⻚表的 Cache，它的作⽤与⻚表相似，但是提⾼了访 问速率。由于采⽤⻚表做地址转换，读写内存数据时 CPU 要访问两次主存。有了快表，有时只 要访问⼀次⾼速缓冲存储器，⼀次主存，这样可加速查找并提⾼指令执⾏速度。
使⽤快表之后的地址转换流程是这样的：

1. 根据虚拟地址中的⻚号查快表；
2. 如果该⻚在快表中，直接从快表中读取相应的物理地址； 
3. 如果该⻚不在快表中，就访问内存中的⻚表，再从⻚表中得到物理地址，同时将⻚表中的该 映射表项添加到快表中；
4. 当快表填满后，⼜要登记新⻚时，就按照⼀定的淘汰策略淘汰掉快表中的⼀个⻚。
   看完了之后你会发现快表和我们平时经常在我们开发的系统使⽤的缓存（⽐如 Redis）很像，的 确是这样的，操作系统中的很多思
5. 想、很多经典的算法，你都可以在我们⽇常开发使⽤的各种⼯ 具或者框架中找到它们的影⼦。

**多级⻚表**
引⼊多级⻚表的主要⽬的是为了避免把全部⻚表⼀直放在内存中占⽤过多空间，特别是那些根本 就不需要的⻚表就不需要保留在内存中。

### 设备管理

#### 磁盘调度算法

1.先来先服务

> FCFS, First Come First Served

按照磁盘请求的顺序进行调度。

优点是公平和简单。缺点也很明显，因为未对寻道做任何优化，使平均寻道时间可能较长。

2.最短寻道时间优先

> SSTF, Shortest Seek Time First

优先调度与当前磁头所在磁道距离最近的磁道。

虽然平均寻道时间比较低，但是不够公平。如果新到达的磁道请求总是比一个在等待的磁道请求近，那么在等待的磁道请求会一直等待下去，也就是出现饥饿现象。具体来说，两端的磁道请求更容易出现饥饿现象。

3. 电梯算法

> SCAN

电梯总是保持一个方向运行，直到该方向没有请求为止，然后改变运行方向。

电梯算法（扫描算法）和电梯的运行过程类似，总是按一个方向来进行磁盘调度，直到该方向上没有未完成的磁盘请求，然后改变方向。

因为考虑了移动方向，因此所有的磁盘请求都会被满足，解决了 SSTF 的饥饿问题。

### I/O

[如何完成一次 IO - 无名鼠辈 (llc687.top)](https://llc687.top/post/如何完成一次-io/)

[IO 模型知多少 | 理论篇 - 「圣杰」 - 博客园 (cnblogs.com)](https://www.cnblogs.com/sheng-jie/p/how-much-you-know-about-io-models.html)

#### **零拷贝**

> 一次 I/O ,无论是读还是写数据，都要经过硬盘 - 内核 - 用户空间，有了 DMA，磁盘到内核空间的拷贝问题得以解决，CPU 可以摸会鱼了。但用户空间和内核空间之间的传输怎么办呢，CPU 觉得要做就做一个摸鱼到下班的 CPU。于是有了零拷贝。
>
> `零拷贝`是基于 DMA 的, 其目的就是优化多次数据拷贝的过程，避免 CPU 将数据从一块存储拷贝到另外一块存储。有 3 个实现思路：
>
> 1. 用户态直接 I/O : 应用程序直接访问硬件存储，内核只辅助数据传输。硬件上的数据直接拷贝给用户空间，也就不存在内核空间缓冲区和用户空间缓冲区间的数据拷贝了。
> 2. 减少数据拷贝次数：在数据传输过程中，减少数据在用户空间缓冲区和系统内核空间缓冲区之间的 CPU 拷贝次数，同时也避免数据在内核空间内部的 CPU 拷贝。
> 3. 写时复制：多个进程共享同一块数据时，如果某进程要对这份数据修改，那将其拷贝到自己的进程地址空间中。

+ **mmap**

  这属于第二类优化，减少了 1 次 CPU 拷贝。MMAP 是数据不会到达用户空间内存，只会存在于系统空间的内存上，用户空间与系统空间共用同一个缓冲区，两者通过映射关联。

  ![screen-1773287](D:\Typora\screen-1773287.png)

  整个mmap过程，发生了4次上下文切换，1次CPU拷贝，2次DMA拷贝

+ **sendfile**

  这也是第二类优化。用户进程不需要单独调用 read/write ，而是直接调用 sendfile() ，sendfile 再帮用户调用 read/write 操作。数据可以直接在内核空间进行 I/O 传输，省去了数据在用户空间和内核空间之间的拷贝。

  与 mmap 内存映射方式不同的是， sendfile() 调用中数据对用户空间是完全不可见的。也就是说，这是一次完全意义上的数据传输过程。

  ![image-20200610154454498](http://img.llc687.top/uPic/image-20200610154454498.png)

  整个过程发生了2次上下文切换，1次CPU拷贝，

#### select、poll、epoll

**IO模型**整体上分为BIO, NIO, IO多路复用, SIGIO, 

select、poll、epoll是IO多路复用的三种关键字

+ **select/poll**

  Select是内核提供的系统调用，它支持一次查询多个系统调用的可用状态，当任意一个结果状态可用时就会返回，用户进程再发起一次系统调用进行数据读取。换句话说，就是NIO中N次的系统调用，借助Select，只需要发起一次系统调用就够了。其IO流程如下所示：

  ![I/O Multiplexing](https://user-gold-cdn.xitu.io/2020/4/14/17175f198bf13c57?w=706&h=381&f=png&s=93272)

  但是，select有一个限制，就是存在连接数限制，针对于此，又提出了poll。其与select相比，主要是解决了连接限制。

  select/epoll 虽然解决了NIO重复无效系统调用用的问题，但同时又引入了新的问题。问题是：

  1. 用户空间和内核空间之间，大量的数据拷贝
  2. 内核循环遍历IO状态，浪费CPU时间

  换句话说，select/poll虽然减少了用户进程的发起的系统调用，但内核的工作量只增不减。在高并发的情况下，内核的性能问题依旧。所以select/poll的问题本质是：内核存在无效的循环遍历。

+ **epoll**

  ![epoll](https://user-gold-cdn.xitu.io/2020/4/14/17175f198d29b33a?w=736&h=390&f=png&s=79697)

  epoll相较于select/poll，多了两次系统调用，其中epoll_create建立与内核的连接，epoll_ctl注册事件，epoll_wait阻塞用户进程，等待IO事件。

  ![select,poll,epoll](https://user-gold-cdn.xitu.io/2020/4/14/17175f198e87ddcc?w=947&h=344&f=png&s=127285)

  epoll，已经大大优化了IO的执行效率，但在IO执行的第一阶段：数据准备阶段都还是被阻塞的。所以这是一个可以继续优化的点。

## Linux

+ 能简单使用 cat，grep，cut 等命令进行一些操作；

+ 文件系统相关的原理，inode 和 block 等概念，数据恢复；

+ 硬链接与软链接；

+ 进程管理相关，僵尸进程与孤儿进程，SIGCHLD 。

  > + 一个子进程的进程描述符在子进程退出时不会释放，只有当父进程通过 wait() 或 waitpid() 获取了子进程信息后才会释放。如果子进程退出，而父进程并没有调用 wait() 或 waitpid()，那么子进程的进程描述符仍然保存在系统中，这种进程称之为僵尸进程。
  >
  >   僵尸进程通过 ps 命令显示出来的状态为 Z（zombie）。
  >
  >   系统所能使用的进程号是有限的，如果产生大量僵尸进程，将因为没有可用的进程号而导致系统不能产生新的进程。
  >
  >   要消灭系统中大量的僵尸进程，只需要将其父进程杀死，此时僵尸进程就会变成孤儿进程，从而被 init 进程所收养，这样 init 进程就会释放所有的僵尸进程所占有的资源，从而结束僵尸进程。

## 计算机网络

### 传输层

#### TCP可靠数据传输

> 可靠数据传输通过校验和(流量控制)、序号、确认、定时器来保证

#### SR（选择重传协议）

> 对于SR协议而言，窗口长度必须小于或等于序号空间大小的一半

#### TCP报文结构段

![TCP报文段首部（图片来源《计算机网络》-谢希仁）](https://img-blog.csdnimg.cn/20190902195359730.png?)

+ 32比特的序号字段和32比特的确认号字段
+ 16比特的接收窗口字段（用于流量控制）
+ 4比特的首部长度字段
+ 可选与变长的选项字段
+ 6比特的标志字段

#### TCP是GBN协议还是SR协议

> TCP确认是累计式的，正确接收但失序的报文段是不会被接收方逐个确认的。因此，TCP发送方仅需维持已发送过但未被确认的字节的最小序号(SendBase)，和下一个要发送的字节的序号（NextSeqNum）
>
> 但是TCP与GBN有显著的区别，在于许多TCP的实现会把正确接受但失序的报文段缓存起来。
>
> 并且考虑例子：当发送方发送的一组报文段1，2，...，N，并且所有报文段都按序无差错地到达接收方时会发生的情况。进一步假设对分组n<N的确认报文丢失，但是其余N-1个确认报文在分别超时以前到达发送端，这时又会发生的情况。在该例中，GBN不仅会重传分组n，还会重传所有后继的分组n+1，n+2，...，N。在另一方面，TCP将重传至多一个报文段，即报文段n。此外，如果报文段n+1的确认报文在报文段n超时之前到达，TCP甚至不会重传报文段n

#### IP数据报的校验和相较于TCP（UDP）报文校验和有什么区别

> UDP首先提供了校验和的功能，（以太网等协议也提供了差错控制）其原因是不能保证源和目的之间的所有链路都提供差错检测。这就是说，也许这些链路中的一条可能使用没有差错检测的协议。此外，即使报文段经链路正确地传输，当报文段存储在某台路由器的内存中时，也可能引入比特差错。
> 在既无法确保链路可靠性，又无法确保内存中的差错检测的情况下。UDP就必须在端到端的基础上在运输层提供差错检测。这是一个在系统设计中被称颂的端到端原则的例子

#### TCP丢包事件

> 我们将一个TCP发送方的“丢包事件”定义为：要么出现超时，要么收到来自接收方的3个冗余ACK

#### TCP/UDP分包

> 以太网 Ethernet 最大的数据帧是 1518字节 。 以太网帧的帧头 14字节和帧尾CRC校验4字节 （共占 18字节 ），剩下承载上层协议的地方也就是Data域最大就只剩1500字节. 这个值我们就把它称之为MTU。
>
> cat /sys/class/net/eth0/mtu
> 1500
>
> 因此单个TCP包 实际传输的最大值：
> 1500- 20（IP头 ）-32（20字节 TCP头和12字节TCP选项时间戳 ） = 1448 字节
>
> 单个UDP包一般不要超过1000字节，不然会发生分包问题

#### TCP拥塞控制

概念：

1. 拥塞窗口cwnd，拥塞窗口有多大，一次性就能传多少个数据报。
2. 拥塞门限ssthresh，当cwnd < ssthresh使用慢开始算法，当cwnd > ssthresh使用拥塞控制算法
3. swnd发送窗口，在发送端初始定义swnd = cwnd = 1；

TCP拥塞控制算法分为慢开始，拥塞避免，快重传，快恢复四个阶段。

1. 慢开始
   1. 在cwnd < ssthresh的阶段，cwnd的增长是指数级变化，如本次能传4个报文并正确接受回复，cwnd就更新为8，以此类推
2. 拥塞避免
   1. 当cwnd > sshtresh时，就进入了拥塞避免阶段，本阶段，cwnd为线性增长。每次+1。
   2. 假如网络发生了超时的现象，系统则判定网络发生了拥塞，此时将ssthresh = ssthresh / 2；cwnd = 1；
3. 快重传
   1. 此阶段为优化系统判定网络拥塞的另一个阶段。也就是**如果接收方连续发送3个ACK，也就是三个要求重传信号**，系统此时不会将cwnd=1（这样会降低传输效率）
   2. 而是将ssthresh /= 2; cwnd = ssthresh;将拥塞窗口更新为拥塞阶段的ssthresh的一半，重新进入拥塞避免算法。
4. 快恢复
   1. 指的也就是快重传后，系统进入拥塞避免算法，cwnd重新开始线性增加。

## HTTP

### session、cookie和token

session与cookie：session通过在服务器上存储session id，以及将session id附在cookie上记录用户状态。

session与token：session是存储session id，token是使用客户端存储的sign通过加密传输到服务端，在服务端计算验证sign是否相等，免去了在机器上存储的问题

> 其实从逻辑上token和sessionid是差不多的，都是关联了一组用户数据。只不过token不通过cookie维持状态（session一般通过cookie维持状态），只根据传入的token作查询或解密验证。

### HTTP状态码

+ 1xx 属于提示信息，是协议处理中的一种中间状态，使用比较少。
+ 2xx 表示服务器成功处理了请求
  + 200 OK 如果是非head请求，将会返回body数据
  + 204 No Content 也是成功的返回码。与200的不同是它没有body数据
  + 206 Partial Content」是应用于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，而 是其中的一部分，也是服务器处理成功的状态
+ 3xx 表示客户端资源发生了变动，需要客户端重新发送URL请求。
  + 301 Moved Permanently 表示**永久重定向**，说明请求的资源已经不存在了，浏览器要用新的URL来访问，也就是**重定向**。
  + 302 Found 表示临时重定向，也就意味着资源还在，但是浏览器暂时需要用另一个URL来访问。
  + 304 Not Modified 缓存重定向。
+ 4xx 报文有误
  + 400 Bad Request 表示客户端请求但报文有误。
  + 401 Forbidden 服务器禁止访问资源，并不是客户端请求出错。
  + 404 Not Found
+ 5xx 服务器错误
  + 500 Internal Server Error」与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。
  + 501 Not Implemented」表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思。
  + 502 Bad Gateway」通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器 发生了错误。
  + 503 Service Unavailable」表示服务器当前很忙，暂时无法响应服务器，类似“网络服务正忙，请稍后􏰁试”的意 思。

### HTTP常见字段

+ Host 服务器域名
  + Host:www.baidu.com
+ Content-Length 数据长度
  + Content-Length:1000
+ Connection 连接方式
  + Connection:keep-alive
+ Content-Type 数据格式
  + Content-Type:text/html;charset = utf-8
+ Content-Encoding 压缩格式
  + Content-Encoding:gzip

### HTTP演进历程

+ HTTP1.0
  + head+body
  + 明文传输安全性差
+ HTTP1.1
  + 增加了长连接功能
  + 管道传输（pipeline）
+ HTTP2.0
  + 

### HTTPS

#### 混合加密

+ 混合加密分为对称加密和非对称加密两种方式。
  + 在通信建立前采用**非对称加密**的方式交换「会话秘钥」，后续就不再使用非对称加密。
  + 在通信过程中全部使用**对称加密**的「会话秘钥」的方式加密明文数据。
+ 采用「混合加密」的方式的原因:
  + 对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。
  + 非对称加密使用两个密钥:公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。

#### 摘要算法

> 摘要算法用来实现完整性，能够为数据生成独一无二的「指纹」，用于校验数据的完整性，解决了篡改的⻛险。客户端在发送明文之前会通过摘要算法算出明文的「指纹」，发送的时候把「指纹 + 明文」一同加密成密文后，发 送给服务器，服务器解密后，用相同的摘要算法算出发送过来的明文，通过比较客户端携带的「指纹」和当前算出 的「指纹」做比较，若「指纹」相同，说明数据是完整的。

#### 数字证书

## Spring

### SpringBean的加载过程

对于Spring加载bean的过程，大致分为以下几步：

 一、转换对应beanName：

 这里传入的参数name不一定就是beanName，有可能是别名或FactoryBean，所以需要进行一系列的解析，这些解析内容包括如下内容：

 1.去除FactoryBean的修饰符，也就是如果name=”&aa”，那么会首先去除&而使name=”aa”

 2.取指定alias所表示的最终beanName，例如别名A指向名称为B的bean则返回B；若别名A指向别名B，别名B又指向名称为C的bean则返回C

 二、尝试从缓存中加载单例：

 单例在Spring的同一个容器内只会被创建一次，后续再获取bean，就直接从单例缓存中获取了。这里只是尝试加载，首先尝试从缓存中加载，如果加载不成功则再次尝试从singletonFactories中加载，因为在创建单例bean的时候会存在依赖注入的情况，而在创建依赖的时候为了避免循环依赖，在Spring中创建bean的原则是不等bean创建完成就会将创建bean的ObjectFactory提早曝光加入到缓存中，一旦下一个bean创建时候需要依赖上一个bean则直接使用ObjectFactory。

 三、bean的实例化：

 如果从缓存中得到了bean的原始状态，则需要对bean进行实例化，这里有必要强调一下，在缓存中记录的只是最原始的bean状态，并不一定是我们最终想要的bean

 四、原型模式的依赖检查

 只有在单例情况下才会尝试解决循环依赖，如果存在A中有B的属性，B中有A的属性，那么当依赖注入的时候，就会产生当A还未创建完的时候因为对于B的创建再次返回创建A，造成循环依赖，也就是情况：isPrototypeCurrentlyInCreation(beanName)判断true 

 五、检测parentBeanFactory 

 从代码上来看，如果缓存没有数据的话直接转到父类工厂上去加载，!this.containsBeanDefinition(beanName检测如果当前加载的XML配置文件中不包含beanName所对应的配置，就只能到parentBeanFactory去尝试，然后再去递归的调用getBean方法 

 六、将存储XML配置文件的GernericBeanDefinition转换为RootBeanDefinition： 

 因为从XML配置文件中读取到的Bean信息是存储在GernericBeanDefinition中的，但是所有的Bean后续处理都是针对于RootBeanDefinition的，所以这里需要进行一个转换，转换的同时如果父类bean不为空的话，则会一并合并父类属性 

 七、寻找依赖 

 因为bean的初始化过程很可能会用到某些属性，而某些属性很可能是动态配置的，并且配置成依赖于其他的bean，那么这个时候就有必要先加载依赖的bean，所以，在Spring的加载顺寻中，在初始化某一个bean的时候首先会初始化这个bean所对应的依赖 

 八、针对不同的scope进行bean的创建 

 在Spring中存在着不同的scope，其中默认的是singleton，但是还有些其他的配置诸如prototype、request之类的，在这个步骤中，Spring会根据不同的配置进行不同的初始化策略 

 九、类型转换 

 程序到这里返回bean后已经基本结束了，通常对该方法的调用参数requiredType是为空的，但是可能会存在这样的情况，返回的bean其实是个Spring，但是requiredType却传入Integer类型，那么这时候本步骤就会起作用了，它的功能是将返回的bean转换为requiredType所指定的类型，当然，Spring转换为Integer是最简单的一种转换，在Spring中提供了各种各样的转换器，用户也可以自己扩展转换器来满足需求。

## 数据库

### MySQL

#### 索引

1、对于sex性别字段要不要建索引的问题？

  一种说法是对于选择性低的列（比如sex）不应当建索引。
  但高性能MySQL中认为，几乎每次查询都会用到sex列。更重要的一点是**如果不需要检索sex列，可以通过IN等值条件查询绕过它，因此给sex上建索引没有坏处。**

  > 绕过的方法为在查询条件中新增AND SEX IN('m','f')
  > 例如建立的联合索引为(sex, country, age), 查询为：SELECT country, age FROM table WHERE sex IN ('m', 'f') AND country = "New York" AND age bewteen 18 and 25;

  需要特别注意的两个点，一个是避免多个IN（）条件，因为每额外增加一个IN（），对优化器的负担很大。第二个是避免多个范围条件，因为同时使用两个及以上的范围查询，不能同时使用到索引。

2、SQL慢查询问题

  针对这个问题需要做的，首先是定位执行慢的SQL语句，其次分析该SQL语句。

  **定位慢查询**有两种方法：

    1. 使用慢查询日志
       1. 开启慢查询日志 set global slow_query_log = on;
       2. 设置慢查询阈值(一般为1s) set global long_query_time = 1;
       3. 确定慢查询日志路径
       4. 确定慢查询日志的文件名
    2. 通过show processlist

  定位到慢查询SQL语句后，**使用Explain来进行分析**。

#### 锁机制

MySQL中的锁分为Record

#### MVCC

##### 可见性算法分析

MVCC中有ReadView（读试图）的概念，这个ReadView会遵循可见性算法。

可见性算法会维护trx_ids, up_limit_id, low_limit_id三个变量

>trx_ids:一个数值列表，用于维护Read View生成时刻系统正活跃的事务ID
>up_limit_id:记录trx_ids中事务ID最小的ID
>low_limit_id:记录Read View生成时刻系统尚未分配的下一个事务ID，也就是**目前已出现过的事务ID的最大值+1**

可见性算法大致流程：

1. 首先比较DB_TRX_ID < up_limit_id, 如果小于，则当前事务能看到DB_TRX_ID 所在的记录，如果大于等于进入下一个判断
2. 接下来判断 DB_TRX_ID 大于等于 low_limit_id , 如果大于等于则代表DB_TRX_ID 所在的记录在Read View生成后才出现的，那对当前事务肯定不可见，如果小于则进入下一个判断
3. 判断DB_TRX_ID 是否在活跃事务之中，trx_list.contains(DB_TRX_ID)，如果在，则代表我Read View生成时刻，你这个事务还在活跃，还没有Commit，你修改的数据，我当前事务也是看不见的；如果不在，则说明，你这个事务在Read View生成之前就已经Commit了，你修改的结果，我当前事务是能看见的

事务中快照读的结果是非常依赖**该事务首次出现快照读的地方**，即某个事务中首次出现快照读的地方非常关键，它有决定该事务后续快照读结果的能力。
总结：如果是**RR隔离级别**，也就是说某个事务中如果有多次快照读，其结果都会依赖于首次出现的情况。如果是**RC隔离级别**，每次快照读都会生成并获取最新的Read View

### Redis

#### 常见数据结构

+ string

  简单的key-value结构，value采用的是Redis自己实现的SDS（简单动态字符串）结构，有查询长度O(1)效率，杜绝缓冲区溢出，减少修改字符串所需的内存重分配次数等优点。

  应用场景：一般用于需要计数的场景，比如用户的访问次数，热点文章的点赞转发数量

+ list

  > list有压缩列表ziplist和双向链表linkedList两种实现。
  > 因为双向链表占用的内存比压缩列表要多， 所以当创建新的列表键时， 列表会优先考虑使用压缩列表， 并且在有需要的时候， 才从压缩列表实现转换到双向链表实现。

  应用场景：发布与订阅或者说消息队列、慢查询

+ hash

  应用场景：系统中对象数据的存储

+ set

  应用场景: 需要存放的数据不能重复以及需要获取多个数据源交集和并集等场景

+ zset（有序集合）

  > zset底层的存储结构包括ziplist或skiplist，在同时满足以下两个条件的时候使用ziplist，其他时候使用skiplist，两个条件如下：
  >
  > + 有序集合保存的元素数量小于128个
  > + 有序集合保存的所有元素的长度小于64字节
  >
  >  当ziplist作为zset的底层存储结构时候，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员，第二个元素保存元素的分值。
  >
  >  当skiplist作为zset的底层存储结构的时候，使用skiplist按序保存元素及分值，使用dict来保存元素和分值的映射关系。

  应用场景：需要对数据根据某个权重进行排序的场景。比如在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维度的消息排行榜）等信息。

#### 跳表

**跳表的实现**
跳表由zskiplistNode和zskiplist两个结构实现，功能分别为表示跳跃表节点，保存跳跃表节点的相关信息。

![image-20210509145631550](D:\Typora\image-20210509145631550.png)

优势:

+ 查询/插入/删除时间复杂度均为O（logn）
+ 相较于红黑树的优势：
  + 在做范围查找的时候，平衡树比skiplist操作要复杂。
  + 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。
  + 从内存占用上来说，skiplist比平衡树更灵活一些。

#### 过期数据的删除策略

+ 定时删除

  > 在设置键的过期时间的同时，创建一个定时器（timer），让定时器在键的过期时间来临时，立即执行对键的删除操作。

+ 惰性删除

  > 只会在取出 key 的时候才对数据进行过期检查。这样对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。

+ 定期删除

  >  每隔一段时间抽取一批 key 执行删除过期 key 操作。并且，Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。

定期删除对内存更加友好，惰性删除对 CPU 更加友好。两者各有千秋，所以 Redis 采用的是 **定期删除+惰性/懒汉式删除** 。

#### 内存淘汰机制

Redis 提供 6 种数据淘汰策略：

1. **volatile-lru（least recently used）**：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰
2. **volatile-ttl**：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰
3. **volatile-random**：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰
4. **allkeys-lru（least recently used）**：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）
5. **allkeys-random**：从数据集（server.db[i].dict）中任意选择数据淘汰
6. **no-eviction**：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！

4.0 版本后增加以下两种：

1. **volatile-lfu（least frequently used）**：从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰
2. **allkeys-lfu（least frequently used）**：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key

#### RDB持久化

##### SAVE和BGSAVE

> 主要区别：SAVE命令由服务器进程执行保存，BGSAVE命令则由子进程执行保存工作。所以SAVE命令会阻塞服务器，而BGSAVE命令则不会。

##### RDB文件结构

![image-20210512162036426](D:\Typora\image-20210512162036426.png)

#### Redis单线程

##### 单线程原因

**官方对于采用单线程设计的回答：**

> 使用Redis时，几乎不存在CPU成为瓶颈的情况， Redis主要受限于内存和网络。例如在一个普通的Linux系统上，Redis通过使用pipelining每秒可以处理100万个请求，所以如果应用程序主要使用O(N)或O(log(N))的命令，它几乎不会占用太多CPU。

**一次完整的客户端与服务器连接事件示例**
![image-20210515153609710](D:\Typora\image-20210515153609710.png)

![image-20210515153627370](D:\Typora\image-20210515153627370.png)

##### Memcached

模型图：

![image-20210515160208261](D:\Typora\image-20210515160208261.png)

> 如上图所示：Memcached 服务器采用 master-woker 模式进行工作，服务端采用 socket 与客户端通讯。主线程、工作线程 采用 pipe管道进行通讯。主线程采用 libevent 监听 listen、accept 的读事件，事件响应后将连接信息的数据结构封装起来，根据算法选择合适的工作线程，将连接任务携带连接信息分发出去，相应的线程利用连接描述符建立与客户端的socket连接 并进行后续的存取数据操作。

#### Redis多线程

**流程简述如下：**

1、主线程负责接收建立连接请求，获取 socket 放入全局等待读处理队列

2、主线程处理完读事件之后，通过 RR(Round Robin) 将这些连接分配给这些 IO 线程

3、主线程阻塞等待 IO 线程读取 socket 完毕

4、主线程通过单线程的方式执行请求命令，请求数据读取并解析完成，但并不执行

5、主线程阻塞等待 IO 线程将数据回写 socket 完毕

6、解除绑定，清空等待队列

**设计特点：**

1、IO 线程要么同时在读 socket，要么同时在写，不会同时读或写

2、IO 线程只负责读写 socket 解析命令，不负责命令处理

#### 旧版复制

##### 同步

同步（sync）

> 当客户端向服务器发送SLAVEOF命令，要求从服务器复制主服务器时，从服务器首先需要执行同步操作，也即是，将从服务器的数据库状态更新至主服务器当前所处的服务器状态。
>
> 从服务器对主服务器的同步操作流程为：
>
> ![image-20210515204143578](D:\Typora\image-20210515204143578.png)

![image-20210515203925157](D:\Typora\image-20210515203925157.png)

##### 命令传播

命令传播（command propagate）

> 问题：同步完成，主从服务器状态样子一致，但每当主服务器执行客户端的写命令时候，主服务器的数据会被修改，并导致主从服务器状态不再一致。
>
> 解决方法：主服务器将执行的写命令发送给从服务器。即命令传播

#### 新版复制

##### PSYNC

![image-20210515204939119](D:\Typora\image-20210515204939119.png)

![image-20210515204959437](D:\Typora\image-20210515204959437.png)

**部分重同步实现关键点**
如下：

+ 主服务器的复制偏移量(replication  offset) 和从服务器的复制偏移量
+ 主服务器的复制积压缓冲区(replication  backlog)
+ 服务器的运行ID

#### Sentinel

Sentinel（哨兵）

**检测主观下线状态和检测客观下线状态**

> **主观下线状态：**Sentinel的配置文件中的down-after-milliseconds选项指定Sentinel判断实例进入主观下线所需的时间长度：**如果在down-after-milliseconds的时间内连续向Sentinel返回无效回复**，则判断已经主观下线。
>
> **客观下线状态**：当Sentinel从其他Sentinel那里接收到足够数量的已下线判断之后，Sentinel就会判定为客观下线。

**is-master-down-by-addr命令**

> ![image-20210517204302392](D:\Typora\image-20210517204302392.png)

**选举Sentinel**

> 该选举过程依据Raft算法实现
>
> 选举过程：
>
> ![image-20210517204603029](D:\Typora\image-20210517204603029.png)
>
> ![image-20210517204618442](D:\Typora\image-20210517204618442.png)

#### 集群

集群的实现

> 集群通过分片（sharding）来进行数据共享，并提供复制和故障转移功能

槽

> Redis集群通过分片的方式来保存数据库中的键值对：集群的整个数据库被分为16384个槽（slot）。
>
> + slots在clusterState.slots和clusterNode.slots中都有出现。（包含关系，Node中包含着State）
>
> + 区别是：clusterState.slots数组记录了集群中所有槽的指派信息，而clusterNode.slots数组中只记录了clusterNode结构所代表的节点的槽指派信息，这是两个slots数组的关键区别所在。
>
> + clusterStates.slots存在的必要性：若想查找槽i被指派给了谁，采用Node.slots查询需要O（N）时间复杂度
>
> + clusterNode.slots存在必要性：
>
>   ![image-20210520195312808](C:\Users\95845\AppData\Roaming\Typora\typora-user-images\image-20210520195312808.png)
>
> + Node.slots数组存储格式为：索引为槽，值为0或1，意思是该节点是否处理这些槽
>
> + State.slots数组结构为：索引为槽，值为Node结构，意思是该槽派个哪个节点

除了将键值对保存在数据库里面之外，节点还会用clusterState结构中的slots_to_keys跳跃表来保存槽和键之间的关系（为了将数据库键和槽联系在一起）；

**[为什么Redis集群有16384个槽](https://www.cnblogs.com/rjzheng/p/11430592.html)**

> The reason is:
>
> - Normal heartbeat packets carry the full configuration of a node, that can be replaced in an idempotent way with the old in order to update an old config. This means they contain the slots configuration for a node, in raw form, that uses 2k of space with16k slots, but would use a prohibitive 8k of space using 65k slots.
> - At the same time it is unlikely that Redis Cluster would scale to more than 1000 mater nodes because of other design tradeoffs.
>
> So 16k was in the right range to ensure enough slots per master with a max of 1000 maters, but a small enough number to propagate the slot configuration as a raw bitmap easily. Note that in small clusters the bitmap would be hard to compress because when N is small the bitmap would have slots/N bits set that is a large percentage of bits set.

+ 主要占空间的是：myslots[CLUSTER_SLOTS/8]，（注意：char字符占一字节，等于8bit）

#### 缓存击穿

定义：大量请求的key根本不存在于缓存上，导致请求直接到了数据库上，根本没有经过缓存那一层。

解决方法：

+ 缓存无效key，并设置较短的过期时间
+ 布隆过滤器

**布隆过滤器**

> **当一个元素加入布隆过滤器中的时候，会进行如下操作：**
>
> 1. 使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。
> 2. 根据得到的哈希值，在位数组中把对应下标的值置为 1。
>
> **当我们需要判断一个元素是否存在于布隆过滤器的时候，会进行如下操作：**
>
> 1. 对给定元素再次进行相同的哈希计算；
> 2. 得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。
>
> #### 缓存雪崩

定义：

+ 场景1：缓存在同一时间大面积失效，后面的请求都直接落到数据库上，造成数据库短时间内承受大量请求。
+ 场景2：有一些热点缓存在某一时刻大面积失效。导致对应的请求直接落在数据库上。

解决方法：

+ 针对Redis服务不可用：
  + 采用Redis集群，避免单机出现问题整个缓存服务都没法使用
  + 限流，避免同时处理大量的请求
+ 针对热点缓存失效的情况
  + 设置不同的失效时间，比如随机设置缓存的失效时间
  + 缓存永不失效

#### RDB与AOF

##### RDB

1、SAVE与BGSAVE
  SAVE命令会让服务主进程阻塞。而BGSAVE会fork一个子进程，后台完成这个RDB持久化的操作。

##### AOF

1、实现形式
  追加命令行

2、AOF的写入与同步
  Redis的服务进程实际上是一个事件循环（loop），执行AOF写命令会让一些内容追加到aof_buff缓存区中，然后根据设置的同步策略进行同步。
  同步策略，也就是appendfsync值，分为：always总是将缓存区所有内容同步到AOF文件中，everysec每秒同步一次（一般是由一个线程单独负责），no不同步根据操作系统的指令进行。

3、AOF的重写机制
  AOF的重写实际上是因为过多的AOF追加命令导致文件过于臃肿，因此会进行重写的操作将部分相同类型的命令合并为一条。
  AOF重写一般是在**子进程**中执行，并且在重写命令执行过程，主进程会维护一个缓存区，待重写完成后，将这期间的写入命令行追加到新AOF文件中。

#### redis多线程

1.Redis6.0之前的版本真的是单线程吗？

> Redis在处理客户端的请求时，包括获取 (socket 读)、解析、执行、内容返回 (socket 写) 等都由一个顺序串行的主线程处理，这就是所谓的“单线程”。但如果严格来讲从Redis4.0之后并不是单线程，除了主线程外，它也有后台线程在处理一些较为缓慢的操作，例如清理脏数据、无用连接的释放、大 key 的删除等等。

2.Redis6.0之前为什么一直不使用多线程？

> 官方曾做过类似问题的回复：使用Redis时，几乎不存在CPU成为瓶颈的情况， Redis主要受限于内存和网络。例如在一个普通的Linux系统上，Redis通过使用pipelining每秒可以处理100万个请求，所以如果应用程序主要使用O(N)或O(log(N))的命令，它几乎不会占用太多CPU。
>
> 使用了单线程后，可维护性高。多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗。Redis通过AE事件模型以及IO多路复用等技术，处理性能非常高，因此没有必要使用多线程。单线程机制使得 Redis 内部实现的复杂度大大降低，Hash 的惰性 Rehash、Lpush 等等 “线程不安全” 的命令都可以无锁进行。

3.Redis6.0为什么要引入多线程呢？

>Redis将所有数据放在内存中，内存的响应时长大约为100纳秒，对于小数据包，Redis服务器可以处理80,000到100,000 QPS，这也是Redis处理的极限了，对于80%的公司来说，单线程的Redis已经足够使用了。
>
>但随着越来越复杂的业务场景，有些公司动不动就上亿的交易量，因此需要更大的QPS。常见的解决方案是在分布式架构中对数据进行分区并采用多个服务器，但该方案有非常大的缺点，例如要管理的Redis服务器太多，维护代价大；某些适用于单个Redis服务器的命令不适用于数据分区；数据分区无法解决热点读/写问题；数据偏斜，重新分配和放大/缩小变得更加复杂等等。
>
>从Redis自身角度来说，因为读写网络的read/write系统调用占用了Redis执行期间大部分CPU时间，瓶颈主要在于网络的 IO 消耗, 优化主要有两个方向:
>
>• 提高网络 IO 性能，典型的实现比如使用 DPDK 来替代内核网络栈的方式
>• 使用多线程充分利用多核，典型的实现比如 Memcached。
>
>协议栈优化的这种方式跟 Redis 关系不大，支持多线程是一种最有效最便捷的操作方式。所以总结起来，redis支持多线程主要就是两个原因：
>
>• 可以充分利用服务器 CPU 资源，目前主线程只能利用一个核
>• 多线程任务可以分摊 Redis 同步 IO 读写负荷

4.Redis6.0多线程的实现机制？

>![Redis多线程处理流程图](https://img-blog.csdnimg.cn/20200505205757213.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0phNW9u,size_16,color_FFFFFF,t_70)
>
>流程简述如下：
>
>1、主线程负责接收建立连接请求，获取 socket 放入全局等待读处理队列
>2、主线程处理完读事件之后，通过 RR(Round Robin) 将这些连接分配给这些 IO 线程
>3、主线程阻塞等待 IO 线程读取 socket 完毕
>4、主线程通过单线程的方式执行请求命令，请求数据读取并解析完成，但并不执行
>5、主线程阻塞等待 IO 线程将数据回写 socket 完毕
>6、解除绑定，清空等待队列
>
>![流程图2](https://img-blog.csdnimg.cn/2020050520583331.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0phNW9u,size_16,color_FFFFFF,t_70)

**总结**：

1. Redis6.0引入多线程是为了解决网络IO即Socket读写的问题。
2. IO线程要么在同时读Socket，要么在同时写Socket，不会出现同时读或写的情况。
3. IO线程只负责网络读写socket解析命令，不负责命令处理。

#### 如何保证缓存与数据库数据一致性

细说的话可以扯很多，但是我觉得其实没太大必要（小声 BB：很多解决方案我也没太弄明白）。我个人觉得引入缓存之后，如果为了短时间的不一致性问题，选择让系统设计变得更加复杂的话，完全没必要。

下面单独对 **Cache Aside Pattern（旁路缓存模式）** 来聊聊。

Cache Aside Pattern 中遇到写请求是这样的：更新 DB，然后直接删除 cache 。

如果更新数据库成功，而删除缓存这一步失败的情况的话，简单说两个解决方案：

1. **缓存失效时间变短（不推荐，治标不治本）** ：我们让缓存数据的过期时间变短，这样的话缓存就会从数据库中加载数据。另外，这种解决办法对于先操作缓存后操作数据库的场景不适用。
2. **增加 cache 更新重试机制（常用）**： 如果 cache 服务当前不可用导致缓存删除失败的话，我们就隔一段时间进行重试，重试次数可以自己定。如果多次重试还是失败的话，我们可以把当前更新失败的 key 存入队列中，等缓存服务可用之后，再将 缓存中对应的 key 删除即可。

> 1）更新数据库的同时为什么不⻢上更新缓存，⽽是删除缓存？
>
> 答：因为考虑到更新数据库后更新缓存可能会因为多线程下导致写⼊脏数据（⽐如线程 A 先更 新数据库成功，接下来要取更新缓存，接着线程 B 更新数据库，但 B ⼜更新了缓存，接着 B 的 时间⽚⽤完了，线程 A 更新了缓存）
>
> 2）更新数据时，是先删除缓存再更新DB，还是先更新DB再删除缓存？
>
> 答：应该先更新DB再删除缓存。因为先删除缓存再更新DB，会产生脏数据。
> 例：两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。

## 分布式

### 注册中心

主流注册中心有Zookeeper、Consul、Eureka、Nacos，以下总结一下服务注册中心设计时须考虑的要点。

#### 数据模型

Zookeeper没有针对服务发现设计数据模型，它的数据是以一种更加抽象的树形K-V组织的，因此理论上可以> 存储任何语义的数据。而Eureka或者Consul都是做到了实例级别的数据扩展，这可以满足大部分的场景，不过无法满足大规模和多环境的服务数据存储。Nacos在经过内部多年生产经验后提炼出的数据模型，则是一种服务-集群-实例的三层模型。如上文所说，这样基本可以满足服务在所有场景下的数据存储和管理。

Nacos的数据模型虽然相对复杂，但是它并不强制你使用它里面的所有数据，在大多数场景下，你可以选择忽略这些数据属性，此时可以降维成和Eureka和Consul一样的数据模型。

另外一个需要考虑的是数据的隔离模型，作为一个共享服务型的组件，需要能够在多个用户或者业务方使用的情况下，保证数据的隔离和安全，这在稍微大一点的业务场景中非常常见。另一方面服务注册中心往往会支持云上部署，此时就要求服务注册中心的数据模型能够适配云上的通用模型。Zookeeper、Consul和Eureka在开源层面都没有很明确的针对服务隔离的模型，Nacos则在一开始就考虑到如何让用户能够以多种维度进行数据隔离，同时能够平滑的迁移到阿里云上对应的商业化产品。

#### 数据一致性

目前来看基本可以归为两家：**一种是基于Leader的非对等部署的单点写一致性，一种是对等部署的多写一致性。**当我们选用服务注册中心的时候，并没有一种协议能够覆盖所有场景，例如当注册的服务节点不会定时发送心跳到注册中心时，强一致协议看起来是唯一的选择，因为无法通过心跳来进行数据的补偿注册，第一次注册就必须保证数据不会丢失。而当客户端会定时发送心跳来汇报健康状态时，第一次的注册的成功率并不是非常关键（当然也很关键，只是相对来说我们容忍数据的少量写失败），因为后续还可以通过心跳再把数据补偿上来，此时Paxos协议的单点瓶颈就会不太划算了，这也是Eureka为什么不采用Paxos协议而采用自定义的Renew机制的原因。

这两种数据一致性协议有各自的使用场景，对服务注册的需求不同，就会导致使用不同的协议。在这里可以发现，Zookeeper在Dubbo体系下表现出的行为，其实采用Eureka的Renew机制更加合适，因为Dubbo服务往Zookeeper注册的就是临时节点，需要定时发心跳到Zookeeper来续约节点，并允许服务下线时，将Zookeeper上相应的节点摘除。Zookeeper使用ZAB协议虽然保证了数据的强一致，但是它的机房容灾能力的缺乏，无法适应一些大型场景。

Nacos因为要支持多种服务类型的注册，并能够具有机房容灾、集群扩展等必不可少的能力，在1.0.0正式支持AP和CP两种一致性协议并存。1.0.0重构了数据的读写和同步逻辑，将与业务相关的CRUD与底层的一致性同步逻辑进行了分层隔离。然后将业务的读写（主要是写，因为读会直接使用业务层的缓存）抽象为Nacos定义的数据类型，调用一致性服务进行数据同步。在决定使用CP还是AP一致性时，使用一个代理，通过可控制的规则进行转发。

目前的一致性协议实现，一个是基于简化的Raft的CP一致性，一个是基于自研协议Distro的AP一致性。Raft协议不必多言，基于Leader进行写入，其CP也并不是严格的，只是能保证一半所见一致，以及数据的丢失概率较小。Distro协议则是参考了内部ConfigServer和开源Eureka，在不借助第三方存储的情况下，实现基本大同小异。Distro重点是做了一些逻辑的优化和性能的调优。

#### 负载均衡

#### 健康检查

Zookeeper和Eureka都实现了一种TTL的机制，就是如果客户端在一定时间内没有向注册中心发送心跳，则会将这个客户端摘除。Eureka做的更好的一点在于它允许在注册服务的时候，自定义检查自身状态的健康检查方法。这在服务实例能够保持心跳上报的场景下，是一种比较好的体验，在Dubbo和SpringCloud这两大体系内，也被培养成用户心智上的默认行为。Nacos也支持这种TTL机制，不过这与ConfigServer在阿里巴巴内部的机制又有一些区别。Nacos目前支持临时实例使用心跳上报方式维持活性，发送心跳的周期默认是5秒，Nacos服务端会在15秒没收到心跳后将实例设置为不健康，在30秒没收到心跳时将这个临时实例摘除。

不过正如前文所说，有一些服务无法上报心跳，但是可以提供一个检测接口，由外部去探测。这样的服务也是广泛存在的，而且以我们的经验，这些服务对服务发现和负载均衡的需求同样强烈。服务端健康检查最常见的方式是TCP端口探测和HTTP接口返回码探测，这两种探测方式因为其协议的通用性可以支持绝大多数的健康检查场景。在其他一些特殊的场景中，可能还需要执行特殊的接口才能判断服务是否可用。例如部署了数据库的主备，数据库的主备可能会在某些情况下切换，需要通过服务名对外提供访问，保证当前访问的库是主库。此时的健康检查接口，可能就是一个检查数据库是否是主库的MYSQL命令了。

#### 集群扩展性

集群扩展性和集群容量以及读写性能关系紧密。当使用一个比较小的集群规模就可以支撑远高于现有数量的服务注册及访问时，集群的扩展能力暂时就不会那么重要。从协议的层面上来说，Zookeeper使用的ZAB协议，由于是单点写，在集群扩展性上不具备优势。Eureka在协议上来说理论上可以扩展到很大规模，因为都是点对点的数据同步，但是从我们对Eureka的运维经验来看，Eureka集群在扩容之后，性能上有很大问题。

集群扩展性的另一个方面是多地域部署和容灾的支持。当讲究集群的高可用和稳定性以及网络上的跨地域延迟要求能够在每个地域都部署集群的时候，我们现有的方案有多机房容灾、异地多活、多数据中心等。

#### Zookeeper与Eureka

Eureka和zookeeper的区别：

1.ZooKeeper保证的是CP,Eureka保证的是AP

2.ZooKeeper有Leader和Follower角色,Eureka各个节点平等

3.ZooKeeper采用过半数存活原则,Eureka采用自我保护机制解决分区问题。因此在zookeeper进行选举和故障转移的过程中服务是不可用的，但是保证了数据的强一致性，而在自我保护机制中服务可用，但不一定是最新数据，当恢复之后会达到最终一致性。

#### Zookeeper不是最佳选择的原因

1. 数据一致性需求分析
   1. Zookeeper为强一致性（CP）架构，在分布式系统中，即使是对等部署的服务，因为请求到达的时间，硬件的状态，操作系统的调度，虚拟机的 GC 等，任何一个时间点，这些对等部署的节点状态也不可能完全一致，而流量不一致的情况下，只要注册中心在 SLA 承诺的时间内（例如 1s 内）将数据收敛到一致状态（即满足最终一致），流量将很快趋于统计学意义上的一致，所以注册中心以最终一致的模型设计在生产实践中完全可以接受。
2. 分区容忍及可用性需求分析
   1. 假设三机房五节点的Zookeeper部署。易知Zookeeper集群必须存活半数以上才可对外提供服务，因此假如存在网络分区的影响，有一个机房不通了，那么就会导致同机房的服务之间出现了无法调用，也就是为了保一致性放弃了高可用，这是绝对不允许的！可以说在实践中，注册中心不能因为自身的任何原因破坏服务之间本身的可连通性，这是注册中心设计应该遵循的铁律！
3. 服务规模、容量、服务连通性
   1. 在服务发现和健康监测场景下，随着服务规模的增大，无论是应用频繁发布时的服务注册带来的写请求，还是刷毫秒级的服务健康状态带来的写请求，还是恨不能整个数据中心的机器或者容器皆与注册中心有长连接带来的连接压力上，ZooKeeper 很快就会力不从心，**而 ZooKeeper 的写并不是可扩展的，不可以通过加节点解决水平扩展性问题。**（由于ZAB协议写，首先每次写入提案需要半数以上同意，延迟很高。其次存在崩溃恢复后数据的对账问题，即崩溃恢复时，会将未提交时候的propasal丢弃，多机房容灾需要人工干预）
4. 持久存储和事务日志
   1. ZooKeeper的ZAB协议对每一个写请求，会在每个 ZooKeeper 节点上保持写一个事务日志，同时再加上定期的将内存数据镜像（Snapshot）到磁盘来保证数据的一致性和持久性，以及宕机之后的数据可恢复，这是非常好的特性，但是我们要问，在服务发现场景中，其最核心的数据 - 实时的健康的服务的地址列表需要数据持久化，但并不需要每次调用的时候都进行关注。
5. 健康检查
   1. Zookeeper采取一刀切方案，就是将服务的健康监测绑定在了 ZooKeeper 对于 Session 的健康监测上，或者说绑定在 TCP 长链接活性探测上了。
6. 容灾考虑
   1. 服务调用（请求响应流）链路应该是弱依赖注册中心，必须仅在服务发布，机器上下线，服务扩缩容等必要时才依赖注册中心。这需要注册中心仔细的设计自己提供的客户端，客户端中应该有针对注册中心服务完全不可用时做容灾的手段，例如设计客户端缓存数据机制（我们称之为 client snapshot）就是行之有效的手段。另外，注册中心的 health check 机制也要仔细设计以便在这种情况不会出现诸如推空等情况的出现。
   2. **ZooKeeper 的原生客户端并没有这种能力**，所以利用 ZooKeeper 实现注册中心的时候我们一定要问自己，如果把 ZooKeeper 所有节点全干掉，你生产上的所有服务调用链路能不受任何影响么？而且应该定期就这一点做故障演练。
7. 生产环境
   1. 生产环境中，注册中心常常部署在云端，由于网络等影响云端master节点的崩溃是频繁性的，而Zookeeper每次重新选举master都无法写入，并且要持续30～90s，时间太久，不适用现在的高并发场景。

### 分布式一致性协议

+ 2PC
+ 3PC
+ Paxos
  + Paxos 算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一，其解决的问题就是在分布式系统中如何就某个值（决议）达成一致 。在 Paxos 中主要有三个角色，分别为 Proposer提案者、Acceptor表决者、Learner学习者。Paxos 算法和 2PC 一样，也有两个阶段，分别为 Prepare 和 accept 阶段。
    prepare 阶段
    + Proposer提案者：负责提出 proposal，每个提案者在提出提案时都会首先获取到一个 具有全局唯一性的、递增的提案编号N，即在整个集群中是唯一的编号 N，然后将该编号赋予其要提出的提案，在第一阶段是只将提案编号发送给所有的表决者。
    + Acceptor表决者：每个表决者在 accept 某提案后，会将该提案编号N记录在本地，这样每个表决者中保存的已经被 accept 的提案中会存在一个编号最大的提案，其编号假设为 maxN。每个表决者仅会 accept 编号大于自己本地 maxN 的提案，在批准提案时表决者会将以前接受过的最大编号的提案作为响应反馈给 Proposer 。
+ ZAB

## 极客-分布式服务

### 多机房部署

+ 同城双活
  制定多机房部署的方案不是一蹴而就的，而是不断迭代发展的。我在上面提到，同城机房之间的延时在 1ms~3ms 左右，对于跨机房调用的容忍度比较高，所以，这种同城双活的方案复杂度会比较低。但是，它只能做到机房级别的容灾，无法做到城市级别的容灾。不过，相比于城市发生地震、洪水等自然灾害来说，机房网络故障、掉电出现的概率要大得多。所以，如果你的系统不需要考虑城市级别的容灾，一般做到同城双活就足够了。

  那么，同城双活的方案要如何设计呢？假设这样的场景：你在北京有 A 和 B 两个机房，A 是联通的机房，B 是电信的机房，机房之间以专线连接，方案设计时，核心思想是尽量避免跨机房的调用。具体方案如下。首先，数据库的主库可以部署在一个机房中，比如部署在 A 机房中，那么 A 和 B 机房数据都会被写入到 A 机房中。然后，在 A、B 两个机房中各部署一个从库，通过主从复制的方式，从主库中同步数据，这样双机房的查询请求可以查询本机房的从库。一旦 A 机房发生故障，可以通过主从切换的方式将 B 机房的从库提升为主库，达到容灾的目的。缓存也可以部署在两个机房中，查询请求也读取本机房的缓存，如果缓存中数据不存在，就穿透到本机房的从库中加载数据。数据的更新可以更新双机房中的数据，保证数据的一致性。

  不同机房的 RPC 服务会向注册中心注册不同的服务组，而不同机房的 RPC 客户端，也就是 Web 服务，只订阅同机房的 RPC 服务组，这样就可以实现 RPC 调用尽量发生在本机房内，避免跨机房的 RPC 调用。
  ![avatar](https://static001.geekbang.org/resource/image/c7/86/c7a4a321ba02cf3ff8c65e9d5bb99686.jpg)

+ 异地多活
  上面这个方案足够应对你目前的需要，但是，你的业务是不断发展的，如果有朝一日，你的电商系统的流量达到了京东或者淘宝的级别，那么你就要考虑即使机房所在的城市发生重大的自然灾害，也要保证系统的可用性。而这时，你需要采用异地多活的方案（据我所知，阿里和饿了么采用的都是异地多活的方案）。

  在考虑异地多活方案时，你首先要考虑异地机房的部署位置。它部署的不能太近，否则发生自然灾害时，很可能会波及。所以，如果你的主机房在北京，那么异地机房就尽量不要建设在天津，而是可以选择上海、广州这样距离较远的位置。但这就会造成更高的数据传输延迟，同城双活中，使用的跨机房写数据库的方案，就不合适了。所以，在数据写入时，你要保证只写本机房的数据存储服务再采取数据同步的方案，将数据同步到异地机房中。

  一般来说，数据同步的方案有两种：一种基于存储系统的主从复制，比如 MySQL 和 Redis。也就是在一个机房部署主库，在异地机房部署从库，两者同步主从复制实现数据的同步。另一种是基于消息队列的方式。一个机房产生写入请求后，会写一条消息到消息队列，另一个机房的应用消费这条消息后再执行业务处理逻辑，写入到存储服务中。我建议你采用两种同步相结合的方式，比如，你可以基于消息的方式，同步缓存的数据、HBase 数据等。然后基于存储，主从复制同步 MySQL、Redis 等数据。无论是采取哪种方案，数据从一个机房传输到另一个机房都会有延迟，所以，你需要尽量保证用户在读取自己的数据时，读取数据主库所在的机房。

  为了达到这一点，你需要对用户做分片，让一个用户每次的读写都尽量在同一个机房中。同时，在数据读取和服务调用时，也要尽量调用本机房的服务。这里有一个场景：假如在电商系统中，用户 A 要查看所有订单的信息，而这些订单中，店铺的信息和卖家的信息很可能是存储在异地机房中，那么你应该优先保证服务调用，和数据读取在本机房中进行，即使读取的是跨机房从库的数据，会有一些延迟，也是可以接受的。
  ![avatar](https://static001.geekbang.org/resource/image/01/73/0138791e6164ea89380f262467820173.jpg)
